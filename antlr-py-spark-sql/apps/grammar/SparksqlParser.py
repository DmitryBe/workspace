# Generated from grammar/Sparksql.g4 by ANTLR 4.5.1
# encoding: utf-8
from antlr4 import *
from io import StringIO

def serializedATN():
    with StringIO() as buf:
        buf.write("\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\3\u0151")
        buf.write("\u02e9\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7")
        buf.write("\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t\13\4\f\t\f\4\r\t\r\4\16")
        buf.write("\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22\4\23\t\23")
        buf.write("\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31")
        buf.write("\t\31\4\32\t\32\4\33\t\33\4\34\t\34\4\35\t\35\4\36\t\36")
        buf.write("\4\37\t\37\4 \t \4!\t!\4\"\t\"\4#\t#\4$\t$\4%\t%\4&\t")
        buf.write("&\4\'\t\'\4(\t(\4)\t)\4*\t*\4+\t+\4,\t,\4-\t-\4.\t.\4")
        buf.write("/\t/\4\60\t\60\4\61\t\61\3\2\3\2\3\3\5\3f\n\3\3\3\3\3")
        buf.write("\5\3j\n\3\3\3\5\3m\n\3\3\4\3\4\3\4\3\5\3\5\3\5\3\5\3\5")
        buf.write("\7\5w\n\5\f\5\16\5z\13\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
        buf.write("\5\3\5\5\5\u0085\n\5\5\5\u0087\n\5\3\6\3\6\5\6\u008b\n")
        buf.write("\6\3\7\3\7\3\7\3\7\3\7\5\7\u0092\n\7\3\7\7\7\u0095\n\7")
        buf.write("\f\7\16\7\u0098\13\7\3\b\3\b\3\b\5\b\u009d\n\b\3\b\3\b")
        buf.write("\3\b\7\b\u00a2\n\b\f\b\16\b\u00a5\13\b\3\t\3\t\3\t\3\t")
        buf.write("\3\t\5\t\u00ac\n\t\3\t\3\t\3\t\3\t\3\t\3\n\3\n\3\n\7\n")
        buf.write("\u00b6\n\n\f\n\16\n\u00b9\13\n\3\13\3\13\5\13\u00bd\n")
        buf.write("\13\3\13\3\13\3\13\5\13\u00c2\n\13\3\13\3\13\5\13\u00c6")
        buf.write("\n\13\5\13\u00c8\n\13\3\13\3\13\3\13\5\13\u00cd\n\13\3")
        buf.write("\13\3\13\3\13\3\13\7\13\u00d3\n\13\f\13\16\13\u00d6\13")
        buf.write("\13\5\13\u00d8\n\13\3\13\3\13\5\13\u00dc\n\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\7\13\u00e3\n\13\f\13\16\13\u00e6\13\13")
        buf.write("\5\13\u00e8\n\13\3\13\3\13\5\13\u00ec\n\13\3\f\3\f\5\f")
        buf.write("\u00f0\n\f\3\f\3\f\5\f\u00f4\n\f\3\f\3\f\3\f\3\f\3\f\6")
        buf.write("\f\u00fb\n\f\r\f\16\f\u00fc\5\f\u00ff\n\f\3\r\3\r\3\r")
        buf.write("\3\r\3\r\5\r\u0106\n\r\3\16\3\16\7\16\u010a\n\16\f\16")
        buf.write("\16\16\u010d\13\16\3\17\3\17\5\17\u0111\n\17\3\17\3\17")
        buf.write("\3\17\5\17\u0116\n\17\5\17\u0118\n\17\3\17\3\17\5\17\u011c")
        buf.write("\n\17\5\17\u011e\n\17\3\20\3\20\3\21\3\21\3\21\3\21\3")
        buf.write("\21\3\21\5\21\u0128\n\21\3\22\3\22\3\22\3\22\3\22\3\22")
        buf.write("\3\22\3\22\3\22\7\22\u0133\n\22\f\22\16\22\u0136\13\22")
        buf.write("\3\23\3\23\3\23\7\23\u013b\n\23\f\23\16\23\u013e\13\23")
        buf.write("\3\24\3\24\3\25\5\25\u0143\n\25\3\25\3\25\5\25\u0147\n")
        buf.write("\25\5\25\u0149\n\25\3\25\5\25\u014c\n\25\3\25\3\25\3\25")
        buf.write("\3\25\3\25\3\25\3\25\3\25\3\25\3\25\3\25\3\25\3\25\3\25")
        buf.write("\5\25\u015c\n\25\3\26\3\26\5\26\u0160\n\26\3\27\5\27\u0163")
        buf.write("\n\27\3\27\3\27\3\30\3\30\5\30\u0169\n\30\3\31\5\31\u016c")
        buf.write("\n\31\3\31\3\31\3\31\3\31\7\31\u0172\n\31\f\31\16\31\u0175")
        buf.write("\13\31\3\31\3\31\3\32\5\32\u017a\n\32\3\32\3\32\3\32\3")
        buf.write("\32\3\32\7\32\u0181\n\32\f\32\16\32\u0184\13\32\3\32\3")
        buf.write("\32\3\32\3\32\3\32\3\32\5\32\u018c\n\32\3\33\3\33\3\34")
        buf.write("\3\34\3\34\3\34\7\34\u0194\n\34\f\34\16\34\u0197\13\34")
        buf.write("\3\34\3\34\3\35\3\35\5\35\u019d\n\35\3\36\3\36\3\36\3")
        buf.write("\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36")
        buf.write("\3\36\3\36\3\36\3\36\3\36\3\36\5\36\u01b3\n\36\3\36\3")
        buf.write("\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36")
        buf.write("\3\36\7\36\u01c2\n\36\f\36\16\36\u01c5\13\36\3\37\3\37")
        buf.write("\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37")
        buf.write("\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37")
        buf.write("\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37")
        buf.write("\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37")
        buf.write("\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37\3\37")
        buf.write("\3\37\3\37\5\37\u0202\n\37\3\37\3\37\3\37\3\37\3\37\5")
        buf.write("\37\u0209\n\37\3\37\5\37\u020c\n\37\3 \5 \u020f\n \3 ")
        buf.write("\3 \3!\3!\3!\3!\5!\u0217\n!\3!\3!\5!\u021b\n!\3!\3!\5")
        buf.write("!\u021f\n!\3\"\3\"\3#\5#\u0224\n#\3#\3#\3$\3$\3$\7$\u022b")
        buf.write("\n$\f$\16$\u022e\13$\3%\3%\3%\5%\u0233\n%\3%\3%\3%\5%")
        buf.write("\u0238\n%\3%\3%\3%\3%\3%\3%\5%\u0240\n%\3%\5%\u0243\n")
        buf.write("%\5%\u0245\n%\3&\3&\3&\5&\u024a\n&\3&\3&\3\'\3\'\3(\3")
        buf.write("(\3(\3(\3(\3(\3(\6(\u0257\n(\r(\16(\u0258\3(\3(\5(\u025d")
        buf.write("\n(\3(\3(\3(\3(\3(\3(\3(\3(\6(\u0267\n(\r(\16(\u0268\3")
        buf.write("(\3(\5(\u026d\n(\3(\3(\5(\u0271\n(\3)\5)\u0274\n)\3)\3")
        buf.write(")\3)\3)\5)\u027a\n)\3)\3)\3*\3*\3*\7*\u0281\n*\f*\16*")
        buf.write("\u0284\13*\3+\3+\3+\7+\u0289\n+\f+\16+\u028c\13+\3,\5")
        buf.write(",\u028f\n,\3,\3,\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3")
        buf.write("-\3-\3-\3-\3-\3-\5-\u02a5\n-\3-\3-\3-\3-\3-\3-\3-\5-\u02ae")
        buf.write("\n-\3-\3-\3-\3-\5-\u02b4\n-\3-\3-\3-\3-\5-\u02ba\n-\3")
        buf.write("-\3-\3-\3-\5-\u02c0\n-\3-\3-\3-\3-\3-\3-\3-\3-\5-\u02ca")
        buf.write("\n-\3.\3.\3.\5.\u02cf\n.\3/\3/\3/\3/\3/\3/\3/\3/\3/\3")
        buf.write("/\3/\3/\3/\3/\3/\5/\u02e0\n/\3\60\3\60\3\61\5\61\u02e5")
        buf.write("\n\61\3\61\3\61\3\61\2\3:\62\2\4\6\b\n\f\16\20\22\24\26")
        buf.write("\30\32\34\36 \"$&(*,.\60\62\64\668:<>@BDFHJLNPRTVXZ\\")
        buf.write("^`\2\21\4\2\u0106\u0106\u0108\u0108\4\2\u00d3\u00d3\u00eb")
        buf.write("\u00eb\4\2\n\n\62\62\4\2\5\5\64\64\5\2KK``\u008d\u008d")
        buf.write("\6\2dd\u00db\u00db\u00e6\u00e6\u0103\u0103\3\2\u012d\u012e")
        buf.write("\3\2\u014c\u014d\3\2\u0149\u014b\4\2\u014c\u014d\u014f")
        buf.write("\u0151\3\2\u0131\u0132\4\2\u012d\u012d\u0131\u0131\4\2")
        buf.write("RR\u0107\u0107\5\2\5\5\b\b\u009d\u009d\4\2\u00bd\u0125")
        buf.write("\u012e\u012e\u0341\2b\3\2\2\2\4e\3\2\2\2\6n\3\2\2\2\b")
        buf.write("q\3\2\2\2\n\u0088\3\2\2\2\f\u0091\3\2\2\2\16\u0099\3\2")
        buf.write("\2\2\20\u00a6\3\2\2\2\22\u00b2\3\2\2\2\24\u00ba\3\2\2")
        buf.write("\2\26\u00f3\3\2\2\2\30\u0105\3\2\2\2\32\u0107\3\2\2\2")
        buf.write("\34\u011d\3\2\2\2\36\u011f\3\2\2\2 \u0127\3\2\2\2\"\u0129")
        buf.write("\3\2\2\2$\u0137\3\2\2\2&\u013f\3\2\2\2(\u015b\3\2\2\2")
        buf.write("*\u015d\3\2\2\2,\u0162\3\2\2\2.\u0166\3\2\2\2\60\u016b")
        buf.write("\3\2\2\2\62\u0179\3\2\2\2\64\u018d\3\2\2\2\66\u018f\3")
        buf.write("\2\2\28\u019c\3\2\2\2:\u01b2\3\2\2\2<\u020b\3\2\2\2>\u020e")
        buf.write("\3\2\2\2@\u021e\3\2\2\2B\u0220\3\2\2\2D\u0223\3\2\2\2")
        buf.write("F\u0227\3\2\2\2H\u0244\3\2\2\2J\u0249\3\2\2\2L\u024d\3")
        buf.write("\2\2\2N\u0270\3\2\2\2P\u0279\3\2\2\2R\u027d\3\2\2\2T\u0285")
        buf.write("\3\2\2\2V\u028e\3\2\2\2X\u02c9\3\2\2\2Z\u02ce\3\2\2\2")
        buf.write("\\\u02df\3\2\2\2^\u02e1\3\2\2\2`\u02e4\3\2\2\2bc\5\4\3")
        buf.write("\2c\3\3\2\2\2df\5\16\b\2ed\3\2\2\2ef\3\2\2\2fg\3\2\2\2")
        buf.write("gi\5\f\7\2hj\5\b\5\2ih\3\2\2\2ij\3\2\2\2jl\3\2\2\2km\5")
        buf.write("\6\4\2lk\3\2\2\2lm\3\2\2\2m\5\3\2\2\2no\7\3\2\2op\5D#")
        buf.write("\2p\7\3\2\2\2qr\7v\2\2rs\7\22\2\2sx\5\n\6\2tu\7\u0146")
        buf.write("\2\2uw\5\n\6\2vt\3\2\2\2wz\3\2\2\2xv\3\2\2\2xy\3\2\2\2")
        buf.write("y\u0086\3\2\2\2zx\3\2\2\2{|\7\u00f2\2\2|}\5:\36\2}\u0084")
        buf.write("\t\2\2\2~\177\7C\2\2\177\u0080\t\3\2\2\u0080\u0081\5:")
        buf.write("\36\2\u0081\u0082\t\2\2\2\u0082\u0083\7\u00f3\2\2\u0083")
        buf.write("\u0085\3\2\2\2\u0084~\3\2\2\2\u0084\u0085\3\2\2\2\u0085")
        buf.write("\u0087\3\2\2\2\u0086{\3\2\2\2\u0086\u0087\3\2\2\2\u0087")
        buf.write("\t\3\2\2\2\u0088\u008a\5:\36\2\u0089\u008b\t\4\2\2\u008a")
        buf.write("\u0089\3\2\2\2\u008a\u008b\3\2\2\2\u008b\13\3\2\2\2\u008c")
        buf.write("\u0092\5\24\13\2\u008d\u008e\7\u0144\2\2\u008e\u008f\5")
        buf.write("\f\7\2\u008f\u0090\7\u0145\2\2\u0090\u0092\3\2\2\2\u0091")
        buf.write("\u008c\3\2\2\2\u0091\u008d\3\2\2\2\u0092\u0096\3\2\2\2")
        buf.write("\u0093\u0095\5\26\f\2\u0094\u0093\3\2\2\2\u0095\u0098")
        buf.write("\3\2\2\2\u0096\u0094\3\2\2\2\u0096\u0097\3\2\2\2\u0097")
        buf.write("\r\3\2\2\2\u0098\u0096\3\2\2\2\u0099\u009c\7\u00ba\2\2")
        buf.write("\u009a\u009b\7\u0125\2\2\u009b\u009d\7\u0146\2\2\u009c")
        buf.write("\u009a\3\2\2\2\u009c\u009d\3\2\2\2\u009d\u009e\3\2\2\2")
        buf.write("\u009e\u00a3\5\20\t\2\u009f\u00a0\7\u0146\2\2\u00a0\u00a2")
        buf.write("\5\20\t\2\u00a1\u009f\3\2\2\2\u00a2\u00a5\3\2\2\2\u00a3")
        buf.write("\u00a1\3\2\2\2\u00a3\u00a4\3\2\2\2\u00a4\17\3\2\2\2\u00a5")
        buf.write("\u00a3\3\2\2\2\u00a6\u00ab\5Z.\2\u00a7\u00a8\7\u0144\2")
        buf.write("\2\u00a8\u00a9\5\22\n\2\u00a9\u00aa\7\u0145\2\2\u00aa")
        buf.write("\u00ac\3\2\2\2\u00ab\u00a7\3\2\2\2\u00ab\u00ac\3\2\2\2")
        buf.write("\u00ac\u00ad\3\2\2\2\u00ad\u00ae\7\t\2\2\u00ae\u00af\7")
        buf.write("\u0144\2\2\u00af\u00b0\5\4\3\2\u00b0\u00b1\7\u0145\2\2")
        buf.write("\u00b1\21\3\2\2\2\u00b2\u00b7\5L\'\2\u00b3\u00b4\7\u0146")
        buf.write("\2\2\u00b4\u00b6\5L\'\2\u00b5\u00b3\3\2\2\2\u00b6\u00b9")
        buf.write("\3\2\2\2\u00b7\u00b5\3\2\2\2\u00b7\u00b8\3\2\2\2\u00b8")
        buf.write("\23\3\2\2\2\u00b9\u00b7\3\2\2\2\u00ba\u00bc\7\u0095\2")
        buf.write("\2\u00bb\u00bd\t\5\2\2\u00bc\u00bb\3\2\2\2\u00bc\u00bd")
        buf.write("\3\2\2\2\u00bd\u00c7\3\2\2\2\u00be\u00bf\7\u00a5\2\2\u00bf")
        buf.write("\u00c1\5:\36\2\u00c0\u00c2\7y\2\2\u00c1\u00c0\3\2\2\2")
        buf.write("\u00c1\u00c2\3\2\2\2\u00c2\u00c5\3\2\2\2\u00c3\u00c4\7")
        buf.write("\u00ba\2\2\u00c4\u00c6\7\u0117\2\2\u00c5\u00c3\3\2\2\2")
        buf.write("\u00c5\u00c6\3\2\2\2\u00c6\u00c8\3\2\2\2\u00c7\u00be\3")
        buf.write("\2\2\2\u00c7\u00c8\3\2\2\2\u00c8\u00c9\3\2\2\2\u00c9\u00cc")
        buf.write("\5F$\2\u00ca\u00cb\7[\2\2\u00cb\u00cd\5P)\2\u00cc\u00ca")
        buf.write("\3\2\2\2\u00cc\u00cd\3\2\2\2\u00cd\u00d7\3\2\2\2\u00ce")
        buf.write("\u00cf\7J\2\2\u00cf\u00d4\5\30\r\2\u00d0\u00d1\7\u0146")
        buf.write("\2\2\u00d1\u00d3\5\30\r\2\u00d2\u00d0\3\2\2\2\u00d3\u00d6")
        buf.write("\3\2\2\2\u00d4\u00d2\3\2\2\2\u00d4\u00d5\3\2\2\2\u00d5")
        buf.write("\u00d8\3\2\2\2\u00d6\u00d4\3\2\2\2\u00d7\u00ce\3\2\2\2")
        buf.write("\u00d7\u00d8\3\2\2\2\u00d8\u00db\3\2\2\2\u00d9\u00da\7")
        buf.write("\u00b8\2\2\u00da\u00dc\5R*\2\u00db\u00d9\3\2\2\2\u00db")
        buf.write("\u00dc\3\2\2\2\u00dc\u00e7\3\2\2\2\u00dd\u00de\7O\2\2")
        buf.write("\u00de\u00df\7\22\2\2\u00df\u00e4\5\36\20\2\u00e0\u00e1")
        buf.write("\7\u0146\2\2\u00e1\u00e3\5\36\20\2\u00e2\u00e0\3\2\2\2")
        buf.write("\u00e3\u00e6\3\2\2\2\u00e4\u00e2\3\2\2\2\u00e4\u00e5\3")
        buf.write("\2\2\2\u00e5\u00e8\3\2\2\2\u00e6\u00e4\3\2\2\2\u00e7\u00dd")
        buf.write("\3\2\2\2\u00e7\u00e8\3\2\2\2\u00e8\u00eb\3\2\2\2\u00e9")
        buf.write("\u00ea\7P\2\2\u00ea\u00ec\5R*\2\u00eb\u00e9\3\2\2\2\u00eb")
        buf.write("\u00ec\3\2\2\2\u00ec\25\3\2\2\2\u00ed\u00ef\7\u00ac\2")
        buf.write("\2\u00ee\u00f0\7\5\2\2\u00ef\u00ee\3\2\2\2\u00ef\u00f0")
        buf.write("\3\2\2\2\u00f0\u00f4\3\2\2\2\u00f1\u00f4\7=\2\2\u00f2")
        buf.write("\u00f4\7Z\2\2\u00f3\u00ed\3\2\2\2\u00f3\u00f1\3\2\2\2")
        buf.write("\u00f3\u00f2\3\2\2\2\u00f4\u00fe\3\2\2\2\u00f5\u00ff\5")
        buf.write("\24\13\2\u00f6\u00f7\7\u0144\2\2\u00f7\u00f8\5\f\7\2\u00f8")
        buf.write("\u00f9\7\u0145\2\2\u00f9\u00fb\3\2\2\2\u00fa\u00f6\3\2")
        buf.write("\2\2\u00fb\u00fc\3\2\2\2\u00fc\u00fa\3\2\2\2\u00fc\u00fd")
        buf.write("\3\2\2\2\u00fd\u00ff\3\2\2\2\u00fe\u00f5\3\2\2\2\u00fe")
        buf.write("\u00fa\3\2\2\2\u00ff\27\3\2\2\2\u0100\u0106\5\32\16\2")
        buf.write("\u0101\u0102\7\u0144\2\2\u0102\u0103\5\32\16\2\u0103\u0104")
        buf.write("\7\u0145\2\2\u0104\u0106\3\2\2\2\u0105\u0100\3\2\2\2\u0105")
        buf.write("\u0101\3\2\2\2\u0106\31\3\2\2\2\u0107\u010b\5\34\17\2")
        buf.write("\u0108\u010a\5(\25\2\u0109\u0108\3\2\2\2\u010a\u010d\3")
        buf.write("\2\2\2\u010b\u0109\3\2\2\2\u010b\u010c\3\2\2\2\u010c\33")
        buf.write("\3\2\2\2\u010d\u010b\3\2\2\2\u010e\u0110\5*\26\2\u010f")
        buf.write("\u0111\5,\27\2\u0110\u010f\3\2\2\2\u0110\u0111\3\2\2\2")
        buf.write("\u0111\u011e\3\2\2\2\u0112\u0117\5 \21\2\u0113\u0115\5")
        buf.write(",\27\2\u0114\u0116\5\66\34\2\u0115\u0114\3\2\2\2\u0115")
        buf.write("\u0116\3\2\2\2\u0116\u0118\3\2\2\2\u0117\u0113\3\2\2\2")
        buf.write("\u0117\u0118\3\2\2\2\u0118\u011e\3\2\2\2\u0119\u011b\7")
        buf.write("\u012c\2\2\u011a\u011c\5,\27\2\u011b\u011a\3\2\2\2\u011b")
        buf.write("\u011c\3\2\2\2\u011c\u011e\3\2\2\2\u011d\u010e\3\2\2\2")
        buf.write("\u011d\u0112\3\2\2\2\u011d\u0119\3\2\2\2\u011e\35\3\2")
        buf.write("\2\2\u011f\u0120\5:\36\2\u0120\37\3\2\2\2\u0121\u0128")
        buf.write("\5&\24\2\u0122\u0123\7\u0144\2\2\u0123\u0124\5&\24\2\u0124")
        buf.write("\u0125\7\u0145\2\2\u0125\u0128\3\2\2\2\u0126\u0128\5\"")
        buf.write("\22\2\u0127\u0121\3\2\2\2\u0127\u0122\3\2\2\2\u0127\u0126")
        buf.write("\3\2\2\2\u0128!\3\2\2\2\u0129\u012a\7\u00b3\2\2\u012a")
        buf.write("\u012b\7\u0144\2\2\u012b\u012c\5$\23\2\u012c\u0134\7\u0145")
        buf.write("\2\2\u012d\u012e\7\u0146\2\2\u012e\u012f\7\u0144\2\2\u012f")
        buf.write("\u0130\5$\23\2\u0130\u0131\7\u0145\2\2\u0131\u0133\3\2")
        buf.write("\2\2\u0132\u012d\3\2\2\2\u0133\u0136\3\2\2\2\u0134\u0132")
        buf.write("\3\2\2\2\u0134\u0135\3\2\2\2\u0135#\3\2\2\2\u0136\u0134")
        buf.write("\3\2\2\2\u0137\u013c\5:\36\2\u0138\u0139\7\u0146\2\2\u0139")
        buf.write("\u013b\5:\36\2\u013a\u0138\3\2\2\2\u013b\u013e\3\2\2\2")
        buf.write("\u013c\u013a\3\2\2\2\u013c\u013d\3\2\2\2\u013d%\3\2\2")
        buf.write("\2\u013e\u013c\3\2\2\2\u013f\u0140\5\4\3\2\u0140\'\3\2")
        buf.write("\2\2\u0141\u0143\7X\2\2\u0142\u0141\3\2\2\2\u0142\u0143")
        buf.write("\3\2\2\2\u0143\u0149\3\2\2\2\u0144\u0146\t\6\2\2\u0145")
        buf.write("\u0147\7w\2\2\u0146\u0145\3\2\2\2\u0146\u0147\3\2\2\2")
        buf.write("\u0147\u0149\3\2\2\2\u0148\u0142\3\2\2\2\u0148\u0144\3")
        buf.write("\2\2\2\u0149\u014b\3\2\2\2\u014a\u014c\t\7\2\2\u014b\u014a")
        buf.write("\3\2\2\2\u014b\u014c\3\2\2\2\u014c\u014d\3\2\2\2\u014d")
        buf.write("\u014e\7]\2\2\u014e\u014f\5\30\r\2\u014f\u0150\7n\2\2")
        buf.write("\u0150\u0151\5R*\2\u0151\u015c\3\2\2\2\u0152\u0153\7$")
        buf.write("\2\2\u0153\u0154\7]\2\2\u0154\u015c\5\30\r\2\u0155\u0156")
        buf.write("\7$\2\2\u0156\u0157\7\u00be\2\2\u0157\u015c\5\30\r\2\u0158")
        buf.write("\u0159\7w\2\2\u0159\u015a\7\u00be\2\2\u015a\u015c\5\30")
        buf.write("\r\2\u015b\u0148\3\2\2\2\u015b\u0152\3\2\2\2\u015b\u0155")
        buf.write("\3\2\2\2\u015b\u0158\3\2\2\2\u015c)\3\2\2\2\u015d\u015f")
        buf.write("\5P)\2\u015e\u0160\5\60\31\2\u015f\u015e\3\2\2\2\u015f")
        buf.write("\u0160\3\2\2\2\u0160+\3\2\2\2\u0161\u0163\7\t\2\2\u0162")
        buf.write("\u0161\3\2\2\2\u0162\u0163\3\2\2\2\u0163\u0164\3\2\2\2")
        buf.write("\u0164\u0165\5.\30\2\u0165-\3\2\2\2\u0166\u0168\5Z.\2")
        buf.write("\u0167\u0169\5\60\31\2\u0168\u0167\3\2\2\2\u0168\u0169")
        buf.write("\3\2\2\2\u0169/\3\2\2\2\u016a\u016c\7\u00ba\2\2\u016b")
        buf.write("\u016a\3\2\2\2\u016b\u016c\3\2\2\2\u016c\u016d\3\2\2\2")
        buf.write("\u016d\u016e\7\u0144\2\2\u016e\u0173\5\62\32\2\u016f\u0170")
        buf.write("\7\u0146\2\2\u0170\u0172\5\62\32\2\u0171\u016f\3\2\2\2")
        buf.write("\u0172\u0175\3\2\2\2\u0173\u0171\3\2\2\2\u0173\u0174\3")
        buf.write("\2\2\2\u0174\u0176\3\2\2\2\u0175\u0173\3\2\2\2\u0176\u0177")
        buf.write("\7\u0145\2\2\u0177\61\3\2\2\2\u0178\u017a\7\u00ee\2\2")
        buf.write("\u0179\u0178\3\2\2\2\u0179\u017a\3\2\2\2\u017a\u018b\3")
        buf.write("\2\2\2\u017b\u017c\7W\2\2\u017c\u017d\7\u0144\2\2\u017d")
        buf.write("\u0182\5\64\33\2\u017e\u017f\7\u0146\2\2\u017f\u0181\5")
        buf.write("\64\33\2\u0180\u017e\3\2\2\2\u0181\u0184\3\2\2\2\u0182")
        buf.write("\u0180\3\2\2\2\u0182\u0183\3\2\2\2\u0183\u0185\3\2\2\2")
        buf.write("\u0184\u0182\3\2\2\2\u0185\u0186\7\u0145\2\2\u0186\u018c")
        buf.write("\3\2\2\2\u0187\u0188\7W\2\2\u0188\u0189\7\u0133\2\2\u0189")
        buf.write("\u018c\5\64\33\2\u018a\u018c\7\u012e\2\2\u018b\u017b\3")
        buf.write("\2\2\2\u018b\u0187\3\2\2\2\u018b\u018a\3\2\2\2\u018c\63")
        buf.write("\3\2\2\2\u018d\u018e\t\b\2\2\u018e\65\3\2\2\2\u018f\u0190")
        buf.write("\7\u0144\2\2\u0190\u0195\58\35\2\u0191\u0192\7\u0146\2")
        buf.write("\2\u0192\u0194\58\35\2\u0193\u0191\3\2\2\2\u0194\u0197")
        buf.write("\3\2\2\2\u0195\u0193\3\2\2\2\u0195\u0196\3\2\2\2\u0196")
        buf.write("\u0198\3\2\2\2\u0197\u0195\3\2\2\2\u0198\u0199\7\u0145")
        buf.write("\2\2\u0199\67\3\2\2\2\u019a\u019d\5Z.\2\u019b\u019d\7")
        buf.write("\u012f\2\2\u019c\u019a\3\2\2\2\u019c\u019b\3\2\2\2\u019d")
        buf.write("9\3\2\2\2\u019e\u019f\b\36\1\2\u019f\u01a0\7\u014e\2\2")
        buf.write("\u01a0\u01b3\5:\36\b\u01a1\u01a2\t\t\2\2\u01a2\u01b3\5")
        buf.write(":\36\6\u01a3\u01b3\7/\2\2\u01a4\u01b3\7i\2\2\u01a5\u01b3")
        buf.write("\7\u012c\2\2\u01a6\u01b3\5@!\2\u01a7\u01b3\5N(\2\u01a8")
        buf.write("\u01b3\5J&\2\u01a9\u01aa\7\u0144\2\2\u01aa\u01ab\5:\36")
        buf.write("\2\u01ab\u01ac\7\u0145\2\2\u01ac\u01b3\3\2\2\2\u01ad\u01ae")
        buf.write("\7\u0144\2\2\u01ae\u01af\5&\24\2\u01af\u01b0\7\u0145\2")
        buf.write("\2\u01b0\u01b3\3\2\2\2\u01b1\u01b3\5<\37\2\u01b2\u019e")
        buf.write("\3\2\2\2\u01b2\u01a1\3\2\2\2\u01b2\u01a3\3\2\2\2\u01b2")
        buf.write("\u01a4\3\2\2\2\u01b2\u01a5\3\2\2\2\u01b2\u01a6\3\2\2\2")
        buf.write("\u01b2\u01a7\3\2\2\2\u01b2\u01a8\3\2\2\2\u01b2\u01a9\3")
        buf.write("\2\2\2\u01b2\u01ad\3\2\2\2\u01b2\u01b1\3\2\2\2\u01b3\u01c3")
        buf.write("\3\2\2\2\u01b4\u01b5\f\7\2\2\u01b5\u01b6\t\n\2\2\u01b6")
        buf.write("\u01c2\5:\36\b\u01b7\u01b8\f\5\2\2\u01b8\u01b9\t\13\2")
        buf.write("\2\u01b9\u01c2\5:\36\6\u01ba\u01bb\f\4\2\2\u01bb\u01bc")
        buf.write("\5\\/\2\u01bc\u01bd\5:\36\5\u01bd\u01c2\3\2\2\2\u01be")
        buf.write("\u01bf\f\r\2\2\u01bf\u01c0\7\32\2\2\u01c0\u01c2\5Z.\2")
        buf.write("\u01c1\u01b4\3\2\2\2\u01c1\u01b7\3\2\2\2\u01c1\u01ba\3")
        buf.write("\2\2\2\u01c1\u01be\3\2\2\2\u01c2\u01c5\3\2\2\2\u01c3\u01c1")
        buf.write("\3\2\2\2\u01c3\u01c4\3\2\2\2\u01c4;\3\2\2\2\u01c5\u01c3")
        buf.write("\3\2\2\2\u01c6\u01c7\7\u00c0\2\2\u01c7\u01c8\7\u0144\2")
        buf.write("\2\u01c8\u01c9\5> \2\u01c9\u01ca\7\u0145\2\2\u01ca\u020c")
        buf.write("\3\2\2\2\u01cb\u01cc\7\u00c5\2\2\u01cc\u01cd\7\u0144\2")
        buf.write("\2\u01cd\u01ce\5> \2\u01ce\u01cf\7\u0145\2\2\u01cf\u020c")
        buf.write("\3\2\2\2\u01d0\u01d1\7\u00d9\2\2\u01d1\u01d2\7\u0144\2")
        buf.write("\2\u01d2\u01d3\5:\36\2\u01d3\u01d4\7\u0145\2\2\u01d4\u020c")
        buf.write("\3\2\2\2\u01d5\u01d6\7\u00da\2\2\u01d6\u01d7\7\u0144\2")
        buf.write("\2\u01d7\u01d8\5$\23\2\u01d8\u01d9\7\u0145\2\2\u01d9\u020c")
        buf.write("\3\2\2\2\u01da\u01db\7\u00e8\2\2\u01db\u01dc\7\u0144\2")
        buf.write("\2\u01dc\u01dd\5> \2\u01dd\u01de\7\u0145\2\2\u01de\u020c")
        buf.write("\3\2\2\2\u01df\u01e0\7\u00e9\2\2\u01e0\u01e1\7\u0144\2")
        buf.write("\2\u01e1\u01e2\5> \2\u01e2\u01e3\7\u0145\2\2\u01e3\u020c")
        buf.write("\3\2\2\2\u01e4\u01e5\7\u0115\2\2\u01e5\u01e6\7\u0144\2")
        buf.write("\2\u01e6\u01e7\5> \2\u01e7\u01e8\7\u0145\2\2\u01e8\u020c")
        buf.write("\3\2\2\2\u01e9\u01ea\7\u0113\2\2\u01ea\u01eb\7\u0144\2")
        buf.write("\2\u01eb\u01ec\5> \2\u01ec\u01ed\7\u0145\2\2\u01ed\u020c")
        buf.write("\3\2\2\2\u01ee\u01ef\7\u0114\2\2\u01ef\u01f0\7\u0144\2")
        buf.write("\2\u01f0\u01f1\5> \2\u01f1\u01f2\7\u0145\2\2\u01f2\u020c")
        buf.write("\3\2\2\2\u01f3\u01f4\7\u0120\2\2\u01f4\u01f5\7\u0144\2")
        buf.write("\2\u01f5\u01f6\5> \2\u01f6\u01f7\7\u0145\2\2\u01f7\u020c")
        buf.write("\3\2\2\2\u01f8\u01f9\7\u0121\2\2\u01f9\u01fa\7\u0144\2")
        buf.write("\2\u01fa\u01fb\5> \2\u01fb\u01fc\7\u0145\2\2\u01fc\u020c")
        buf.write("\3\2\2\2\u01fd\u01fe\7\u00c9\2\2\u01fe\u0201\7\u0144\2")
        buf.write("\2\u01ff\u0202\7\u0149\2\2\u0200\u0202\5> \2\u0201\u01ff")
        buf.write("\3\2\2\2\u0201\u0200\3\2\2\2\u0202\u0203\3\2\2\2\u0203")
        buf.write("\u020c\7\u0145\2\2\u0204\u0205\7\u00ca\2\2\u0205\u0208")
        buf.write("\7\u0144\2\2\u0206\u0209\7\u0149\2\2\u0207\u0209\5> \2")
        buf.write("\u0208\u0206\3\2\2\2\u0208\u0207\3\2\2\2\u0209\u020a\3")
        buf.write("\2\2\2\u020a\u020c\7\u0145\2\2\u020b\u01c6\3\2\2\2\u020b")
        buf.write("\u01cb\3\2\2\2\u020b\u01d0\3\2\2\2\u020b\u01d5\3\2\2\2")
        buf.write("\u020b\u01da\3\2\2\2\u020b\u01df\3\2\2\2\u020b\u01e4\3")
        buf.write("\2\2\2\u020b\u01e9\3\2\2\2\u020b\u01ee\3\2\2\2\u020b\u01f3")
        buf.write("\3\2\2\2\u020b\u01f8\3\2\2\2\u020b\u01fd\3\2\2\2\u020b")
        buf.write("\u0204\3\2\2\2\u020c=\3\2\2\2\u020d\u020f\t\5\2\2\u020e")
        buf.write("\u020d\3\2\2\2\u020e\u020f\3\2\2\2\u020f\u0210\3\2\2\2")
        buf.write("\u0210\u0211\5:\36\2\u0211?\3\2\2\2\u0212\u021f\7\u012f")
        buf.write("\2\2\u0213\u021f\7\u0130\2\2\u0214\u021f\5D#\2\u0215\u0217")
        buf.write("\5B\"\2\u0216\u0215\3\2\2\2\u0216\u0217\3\2\2\2\u0217")
        buf.write("\u0218\3\2\2\2\u0218\u021f\t\f\2\2\u0219\u021b\5B\"\2")
        buf.write("\u021a\u0219\3\2\2\2\u021a\u021b\3\2\2\2\u021b\u021c\3")
        buf.write("\2\2\2\u021c\u021d\7\u0143\2\2\u021d\u021f\t\r\2\2\u021e")
        buf.write("\u0212\3\2\2\2\u021e\u0213\3\2\2\2\u021e\u0214\3\2\2\2")
        buf.write("\u021e\u0216\3\2\2\2\u021e\u021a\3\2\2\2\u021fA\3\2\2")
        buf.write("\2\u0220\u0221\t\t\2\2\u0221C\3\2\2\2\u0222\u0224\5B\"")
        buf.write("\2\u0223\u0222\3\2\2\2\u0223\u0224\3\2\2\2\u0224\u0225")
        buf.write("\3\2\2\2\u0225\u0226\7\u012d\2\2\u0226E\3\2\2\2\u0227")
        buf.write("\u022c\5H%\2\u0228\u0229\7\u0146\2\2\u0229\u022b\5H%\2")
        buf.write("\u022a\u0228\3\2\2\2\u022b\u022e\3\2\2\2\u022c\u022a\3")
        buf.write("\2\2\2\u022c\u022d\3\2\2\2\u022dG\3\2\2\2\u022e\u022c")
        buf.write("\3\2\2\2\u022f\u0230\5P)\2\u0230\u0231\7\u013f\2\2\u0231")
        buf.write("\u0233\3\2\2\2\u0232\u022f\3\2\2\2\u0232\u0233\3\2\2\2")
        buf.write("\u0233\u0237\3\2\2\2\u0234\u0238\7\u0149\2\2\u0235\u0236")
        buf.write("\7\u0143\2\2\u0236\u0238\t\16\2\2\u0237\u0234\3\2\2\2")
        buf.write("\u0237\u0235\3\2\2\2\u0238\u0245\3\2\2\2\u0239\u023a\5")
        buf.write("8\35\2\u023a\u023b\7\u0133\2\2\u023b\u023c\5:\36\2\u023c")
        buf.write("\u0245\3\2\2\2\u023d\u0242\5:\36\2\u023e\u0240\7\t\2\2")
        buf.write("\u023f\u023e\3\2\2\2\u023f\u0240\3\2\2\2\u0240\u0241\3")
        buf.write("\2\2\2\u0241\u0243\58\35\2\u0242\u023f\3\2\2\2\u0242\u0243")
        buf.write("\3\2\2\2\u0243\u0245\3\2\2\2\u0244\u0232\3\2\2\2\u0244")
        buf.write("\u0239\3\2\2\2\u0244\u023d\3\2\2\2\u0245I\3\2\2\2\u0246")
        buf.write("\u0247\5P)\2\u0247\u0248\7\u013f\2\2\u0248\u024a\3\2\2")
        buf.write("\2\u0249\u0246\3\2\2\2\u0249\u024a\3\2\2\2\u024a\u024b")
        buf.write("\3\2\2\2\u024b\u024c\5L\'\2\u024cK\3\2\2\2\u024d\u024e")
        buf.write("\5Z.\2\u024eM\3\2\2\2\u024f\u0250\7\24\2\2\u0250\u0256")
        buf.write("\5:\36\2\u0251\u0252\7\u00b7\2\2\u0252\u0253\5:\36\2\u0253")
        buf.write("\u0254\7\u00a3\2\2\u0254\u0255\5:\36\2\u0255\u0257\3\2")
        buf.write("\2\2\u0256\u0251\3\2\2\2\u0257\u0258\3\2\2\2\u0258\u0256")
        buf.write("\3\2\2\2\u0258\u0259\3\2\2\2\u0259\u025c\3\2\2\2\u025a")
        buf.write("\u025b\79\2\2\u025b\u025d\5:\36\2\u025c\u025a\3\2\2\2")
        buf.write("\u025c\u025d\3\2\2\2\u025d\u025e\3\2\2\2\u025e\u025f\7")
        buf.write(":\2\2\u025f\u0271\3\2\2\2\u0260\u0266\7\24\2\2\u0261\u0262")
        buf.write("\7\u00b7\2\2\u0262\u0263\5R*\2\u0263\u0264\7\u00a3\2\2")
        buf.write("\u0264\u0265\5:\36\2\u0265\u0267\3\2\2\2\u0266\u0261\3")
        buf.write("\2\2\2\u0267\u0268\3\2\2\2\u0268\u0266\3\2\2\2\u0268\u0269")
        buf.write("\3\2\2\2\u0269\u026c\3\2\2\2\u026a\u026b\79\2\2\u026b")
        buf.write("\u026d\5:\36\2\u026c\u026a\3\2\2\2\u026c\u026d\3\2\2\2")
        buf.write("\u026d\u026e\3\2\2\2\u026e\u026f\7:\2\2\u026f\u0271\3")
        buf.write("\2\2\2\u0270\u024f\3\2\2\2\u0270\u0260\3\2\2\2\u0271O")
        buf.write("\3\2\2\2\u0272\u0274\5Z.\2\u0273\u0272\3\2\2\2\u0273\u0274")
        buf.write("\3\2\2\2\u0274\u0275\3\2\2\2\u0275\u027a\7\u013f\2\2\u0276")
        buf.write("\u0277\5Z.\2\u0277\u0278\7\u013f\2\2\u0278\u027a\3\2\2")
        buf.write("\2\u0279\u0273\3\2\2\2\u0279\u0276\3\2\2\2\u0279\u027a")
        buf.write("\3\2\2\2\u027a\u027b\3\2\2\2\u027b\u027c\5Z.\2\u027cQ")
        buf.write("\3\2\2\2\u027d\u0282\5T+\2\u027e\u027f\7\7\2\2\u027f\u0281")
        buf.write("\5T+\2\u0280\u027e\3\2\2\2\u0281\u0284\3\2\2\2\u0282\u0280")
        buf.write("\3\2\2\2\u0282\u0283\3\2\2\2\u0283S\3\2\2\2\u0284\u0282")
        buf.write("\3\2\2\2\u0285\u028a\5V,\2\u0286\u0287\7u\2\2\u0287\u0289")
        buf.write("\5V,\2\u0288\u0286\3\2\2\2\u0289\u028c\3\2\2\2\u028a\u0288")
        buf.write("\3\2\2\2\u028a\u028b\3\2\2\2\u028bU\3\2\2\2\u028c\u028a")
        buf.write("\3\2\2\2\u028d\u028f\7h\2\2\u028e\u028d\3\2\2\2\u028e")
        buf.write("\u028f\3\2\2\2\u028f\u0290\3\2\2\2\u0290\u0291\5X-\2\u0291")
        buf.write("W\3\2\2\2\u0292\u0293\7@\2\2\u0293\u0294\7\u0144\2\2\u0294")
        buf.write("\u0295\5&\24\2\u0295\u0296\7\u0145\2\2\u0296\u02ca\3\2")
        buf.write("\2\2\u0297\u0298\5:\36\2\u0298\u0299\5\\/\2\u0299\u029a")
        buf.write("\5:\36\2\u029a\u02ca\3\2\2\2\u029b\u029c\5:\36\2\u029c")
        buf.write("\u029d\5\\/\2\u029d\u029e\t\17\2\2\u029e\u029f\7\u0144")
        buf.write("\2\2\u029f\u02a0\5&\24\2\u02a0\u02a1\7\u0145\2\2\u02a1")
        buf.write("\u02ca\3\2\2\2\u02a2\u02a4\5:\36\2\u02a3\u02a5\7h\2\2")
        buf.write("\u02a4\u02a3\3\2\2\2\u02a4\u02a5\3\2\2\2\u02a5\u02a6\3")
        buf.write("\2\2\2\u02a6\u02a7\7\16\2\2\u02a7\u02a8\5:\36\2\u02a8")
        buf.write("\u02a9\7\7\2\2\u02a9\u02aa\5:\36\2\u02aa\u02ca\3\2\2\2")
        buf.write("\u02ab\u02ad\5:\36\2\u02ac\u02ae\7h\2\2\u02ad\u02ac\3")
        buf.write("\2\2\2\u02ad\u02ae\3\2\2\2\u02ae\u02af\3\2\2\2\u02af\u02b0")
        buf.write("\7V\2\2\u02b0\u02b3\7\u0144\2\2\u02b1\u02b4\5&\24\2\u02b2")
        buf.write("\u02b4\5$\23\2\u02b3\u02b1\3\2\2\2\u02b3\u02b2\3\2\2\2")
        buf.write("\u02b4\u02b5\3\2\2\2\u02b5\u02b6\7\u0145\2\2\u02b6\u02ca")
        buf.write("\3\2\2\2\u02b7\u02b9\5:\36\2\u02b8\u02ba\7h\2\2\u02b9")
        buf.write("\u02b8\3\2\2\2\u02b9\u02ba\3\2\2\2\u02ba\u02bb\3\2\2\2")
        buf.write("\u02bb\u02bc\7a\2\2\u02bc\u02bf\5:\36\2\u02bd\u02be\7")
        buf.write("<\2\2\u02be\u02c0\5:\36\2\u02bf\u02bd\3\2\2\2\u02bf\u02c0")
        buf.write("\3\2\2\2\u02c0\u02ca\3\2\2\2\u02c1\u02c2\5:\36\2\u02c2")
        buf.write("\u02c3\7\\\2\2\u02c3\u02c4\5`\61\2\u02c4\u02ca\3\2\2\2")
        buf.write("\u02c5\u02c6\7\u0144\2\2\u02c6\u02c7\5R*\2\u02c7\u02c8")
        buf.write("\7\u0145\2\2\u02c8\u02ca\3\2\2\2\u02c9\u0292\3\2\2\2\u02c9")
        buf.write("\u0297\3\2\2\2\u02c9\u029b\3\2\2\2\u02c9\u02a2\3\2\2\2")
        buf.write("\u02c9\u02ab\3\2\2\2\u02c9\u02b7\3\2\2\2\u02c9\u02c1\3")
        buf.write("\2\2\2\u02c9\u02c5\3\2\2\2\u02caY\3\2\2\2\u02cb\u02cf")
        buf.write("\5^\60\2\u02cc\u02cf\7\u012a\2\2\u02cd\u02cf\7\u012b\2")
        buf.write("\2\u02ce\u02cb\3\2\2\2\u02ce\u02cc\3\2\2\2\u02ce\u02cd")
        buf.write("\3\2\2\2\u02cf[\3\2\2\2\u02d0\u02e0\7\u0133\2\2\u02d1")
        buf.write("\u02e0\7\u0134\2\2\u02d2\u02e0\7\u0135\2\2\u02d3\u02d4")
        buf.write("\7\u0135\2\2\u02d4\u02e0\7\u0133\2\2\u02d5\u02d6\7\u0134")
        buf.write("\2\2\u02d6\u02e0\7\u0133\2\2\u02d7\u02d8\7\u0135\2\2\u02d8")
        buf.write("\u02e0\7\u0134\2\2\u02d9\u02da\7\u0136\2\2\u02da\u02e0")
        buf.write("\7\u0133\2\2\u02db\u02dc\7\u0136\2\2\u02dc\u02e0\7\u0134")
        buf.write("\2\2\u02dd\u02de\7\u0136\2\2\u02de\u02e0\7\u0135\2\2\u02df")
        buf.write("\u02d0\3\2\2\2\u02df\u02d1\3\2\2\2\u02df\u02d2\3\2\2\2")
        buf.write("\u02df\u02d3\3\2\2\2\u02df\u02d5\3\2\2\2\u02df\u02d7\3")
        buf.write("\2\2\2\u02df\u02d9\3\2\2\2\u02df\u02db\3\2\2\2\u02df\u02dd")
        buf.write("\3\2\2\2\u02e0]\3\2\2\2\u02e1\u02e2\t\20\2\2\u02e2_\3")
        buf.write("\2\2\2\u02e3\u02e5\7h\2\2\u02e4\u02e3\3\2\2\2\u02e4\u02e5")
        buf.write("\3\2\2\2\u02e5\u02e6\3\2\2\2\u02e6\u02e7\7i\2\2\u02e7")
        buf.write("a\3\2\2\2\\eilx\u0084\u0086\u008a\u0091\u0096\u009c\u00a3")
        buf.write("\u00ab\u00b7\u00bc\u00c1\u00c5\u00c7\u00cc\u00d4\u00d7")
        buf.write("\u00db\u00e4\u00e7\u00eb\u00ef\u00f3\u00fc\u00fe\u0105")
        buf.write("\u010b\u0110\u0115\u0117\u011b\u011d\u0127\u0134\u013c")
        buf.write("\u0142\u0146\u0148\u014b\u015b\u015f\u0162\u0168\u016b")
        buf.write("\u0173\u0179\u0182\u018b\u0195\u019c\u01b2\u01c1\u01c3")
        buf.write("\u0201\u0208\u020b\u020e\u0216\u021a\u021e\u0223\u022c")
        buf.write("\u0232\u0237\u023f\u0242\u0244\u0249\u0258\u025c\u0268")
        buf.write("\u026c\u0270\u0273\u0279\u0282\u028a\u028e\u02a4\u02ad")
        buf.write("\u02b3\u02b9\u02bf\u02c9\u02ce\u02df\u02e4")
        return buf.getvalue()


class SparksqlParser ( Parser ):

    grammarFileName = "Sparksql.g4"

    atn = ATNDeserializer().deserialize(serializedATN())

    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]

    sharedContextCache = PredictionContextCache()

    literalNames = [ "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "'='", "'>'", "'<'", "'!'", "'+='", "'-='", 
                     "'*='", "'/='", "'%='", "'&='", "'^='", "'|='", "'.'", 
                     "'_'", "'@'", "'#'", "'$'", "'('", "')'", "','", "';'", 
                     "':'", "'*'", "'/'", "'%'", "'+'", "'-'", "'~'", "'|'", 
                     "'&'", "'^'" ]

    symbolicNames = [ "<INVALID>", "LIMIT", "ADD", "ALL", "ALTER", "AND", 
                      "ANY", "AS", "ASC", "AUTHORIZATION", "BACKUP", "BEGIN", 
                      "BETWEEN", "BREAK", "BROWSE", "BULK", "BY", "CASCADE", 
                      "CASE", "CHECK", "CHECKPOINT", "CLOSE", "CLUSTERED", 
                      "COALESCE", "COLLATE", "COLUMN", "COMMIT", "COMPUTE", 
                      "CONSTRAINT", "CONTAINS", "CONTAINSTABLE", "CONTINUE", 
                      "CONVERT", "CREATE", "CROSS", "CURRENT", "CURRENT_DATE", 
                      "CURRENT_TIME", "CURRENT_TIMESTAMP", "CURRENT_USER", 
                      "CURSOR", "DATABASE", "DBCC", "DEALLOCATE", "DECLARE", 
                      "DEFAULT", "DELETE", "DENY", "DESC", "DISK", "DISTINCT", 
                      "DISTRIBUTED", "DOUBLE", "DROP", "DUMP", "ELSE", "END", 
                      "ERRLVL", "ESCAPE", "EXCEPT", "EXEC", "EXECUTE", "EXISTS", 
                      "EXIT", "EXTERNAL", "FETCH", "FILE", "FILLFACTOR", 
                      "FOR", "FOREIGN", "FREETEXT", "FREETEXTTABLE", "FROM", 
                      "FULL", "FUNCTION", "GOTO", "GRANT", "GROUP", "HAVING", 
                      "HOLDLOCK", "IDENTITY", "IDENTITYCOL", "IDENTITY_INSERT", 
                      "IF", "IN", "INDEX", "INNER", "INSERT", "INTERSECT", 
                      "INTO", "IS", "JOIN", "KEY", "KILL", "LEFT", "LIKE", 
                      "LINENO", "LOAD", "MERGE", "NATIONAL", "NOCHECK", 
                      "NONCLUSTERED", "NOT", "NULL", "NULLIF", "OF", "OFF", 
                      "OFFSETS", "ON", "OPEN", "OPENDATASOURCE", "OPENQUERY", 
                      "OPENROWSET", "OPENXML", "OPTION", "OR", "ORDER", 
                      "OUTER", "OVER", "PERCENT", "PIVOT", "PLAN", "PRECISION", 
                      "PRIMARY", "PRINT", "PROC", "PROCEDURE", "PUBLIC", 
                      "RAISERROR", "READ", "READTEXT", "RECONFIGURE", "REFERENCES", 
                      "REPLICATION", "RESTORE", "RESTRICT", "RETURN", "REVERT", 
                      "REVOKE", "RIGHT", "ROLLBACK", "ROWCOUNT", "ROWGUIDCOL", 
                      "RULE", "SAVE", "SCHEMA", "SECURITYAUDIT", "SELECT", 
                      "SEMANTICKEYPHRASETABLE", "SEMANTICSIMILARITYDETAILSTABLE", 
                      "SEMANTICSIMILARITYTABLE", "SESSION_USER", "SET", 
                      "SETUSER", "SHUTDOWN", "SOME", "STATISTICS", "SYSTEM_USER", 
                      "TABLE", "TABLESAMPLE", "TEXTSIZE", "THEN", "TO", 
                      "TOP", "TRAN", "TRANSACTION", "TRIGGER", "TRUNCATE", 
                      "TRY_CONVERT", "TSEQUAL", "UNION", "UNIQUE", "UNPIVOT", 
                      "UPDATE", "UPDATETEXT", "USE", "USER", "VALUES", "VARYING", 
                      "VIEW", "WAITFOR", "WHEN", "WHERE", "WHILE", "WITH", 
                      "WITHIN", "WRITETEXT", "ABSOLUTE", "APPLY", "AUTO", 
                      "AVG", "BASE64", "CALLER", "CAST", "CATCH", "CHECKSUM_AGG", 
                      "COMMITTED", "CONCAT", "COOKIE", "COUNT", "COUNT_BIG", 
                      "DELAY", "DELETED", "DENSE_RANK", "DISABLE", "DYNAMIC", 
                      "ENCRYPTION", "FAST", "FAST_FORWARD", "FIRST", "FOLLOWING", 
                      "FORWARD_ONLY", "FULLSCAN", "GLOBAL", "GO", "GROUPING", 
                      "GROUPING_ID", "HASH", "INSENSITIVE", "INSERTED", 
                      "ISOLATION", "KEEPFIXED", "KEYSET", "LAST", "LEVEL", 
                      "LOCAL", "LOCK_ESCALATION", "LOGIN", "LOOP", "MARK", 
                      "MAX", "MIN", "MODIFY", "NEXT", "NAME", "NOCOUNT", 
                      "NOEXPAND", "NORECOMPUTE", "NTILE", "NUMBER", "OFFSET", 
                      "ONLY", "OPTIMISTIC", "OPTIMIZE", "OUT", "OUTPUT", 
                      "OWNER", "PARTITION", "PATH", "PRECEDING", "PRIOR", 
                      "RANGE", "RANK", "READONLY", "READ_ONLY", "RECOMPILE", 
                      "RELATIVE", "REMOTE", "REPEATABLE", "ROOT", "ROW", 
                      "ROWGUID", "ROWS", "ROW_NUMBER", "SAMPLE", "SCHEMABINDING", 
                      "SCROLL", "SCROLL_LOCKS", "SELF", "SERIALIZABLE", 
                      "SNAPSHOT", "STATIC", "STATS_STREAM", "STDEV", "STDEVP", 
                      "SUM", "THROW", "TIES", "TIME", "TRY", "TYPE", "TYPE_WARNING", 
                      "UNBOUNDED", "UNCOMMITTED", "UNKNOWN", "USING", "VAR", 
                      "VARP", "VIEW_METADATA", "WORK", "XML", "XMLNAMESPACES", 
                      "DOLLAR_ACTION", "SPACE", "COMMENT", "LINE_COMMENT", 
                      "DOUBLE_QUOTE_ID", "SQUARE_BRACKET_ID", "LOCAL_ID", 
                      "DECIMAL", "ID", "STRING", "BINARY", "FLOAT", "REAL", 
                      "EQUAL", "GREATER", "LESS", "EXCLAMATION", "PLUS_ASSIGN", 
                      "MINUS_ASSIGN", "MULT_ASSIGN", "DIV_ASSIGN", "MOD_ASSIGN", 
                      "AND_ASSIGN", "XOR_ASSIGN", "OR_ASSIGN", "DOT", "UNDERLINE", 
                      "AT", "SHARP", "DOLLAR", "LR_BRACKET", "RR_BRACKET", 
                      "COMMA", "SEMI", "COLON", "STAR", "DIVIDE", "MODULE", 
                      "PLUS", "MINUS", "BIT_NOT", "BIT_OR", "BIT_AND", "BIT_XOR" ]

    RULE_root = 0
    RULE_select_statement = 1
    RULE_limit_clause = 2
    RULE_order_by_clause = 3
    RULE_order_by_expression = 4
    RULE_query_expression = 5
    RULE_with_expression = 6
    RULE_common_table_expression = 7
    RULE_column_name_list = 8
    RULE_query_specification = 9
    RULE_union = 10
    RULE_table_source = 11
    RULE_table_source_item_joined = 12
    RULE_table_source_item = 13
    RULE_group_by_item = 14
    RULE_derived_table = 15
    RULE_table_value_constructor = 16
    RULE_expression_list = 17
    RULE_subquery = 18
    RULE_join_part = 19
    RULE_table_name_with_hint = 20
    RULE_as_table_alias = 21
    RULE_table_alias = 22
    RULE_with_table_hints = 23
    RULE_table_hint = 24
    RULE_index_value = 25
    RULE_column_alias_list = 26
    RULE_column_alias = 27
    RULE_expression = 28
    RULE_aggregate_windowed_function = 29
    RULE_all_distinct_expression = 30
    RULE_constant = 31
    RULE_sign = 32
    RULE_number = 33
    RULE_select_list = 34
    RULE_select_list_elem = 35
    RULE_full_column_name = 36
    RULE_column_name = 37
    RULE_case_expr = 38
    RULE_table_name = 39
    RULE_search_condition = 40
    RULE_search_condition_or = 41
    RULE_search_condition_not = 42
    RULE_predicate = 43
    RULE_id_1 = 44
    RULE_comparison_operator = 45
    RULE_simple_id = 46
    RULE_null_notnull = 47

    ruleNames =  [ "root", "select_statement", "limit_clause", "order_by_clause", 
                   "order_by_expression", "query_expression", "with_expression", 
                   "common_table_expression", "column_name_list", "query_specification", 
                   "union", "table_source", "table_source_item_joined", 
                   "table_source_item", "group_by_item", "derived_table", 
                   "table_value_constructor", "expression_list", "subquery", 
                   "join_part", "table_name_with_hint", "as_table_alias", 
                   "table_alias", "with_table_hints", "table_hint", "index_value", 
                   "column_alias_list", "column_alias", "expression", "aggregate_windowed_function", 
                   "all_distinct_expression", "constant", "sign", "number", 
                   "select_list", "select_list_elem", "full_column_name", 
                   "column_name", "case_expr", "table_name", "search_condition", 
                   "search_condition_or", "search_condition_not", "predicate", 
                   "id_1", "comparison_operator", "simple_id", "null_notnull" ]

    EOF = Token.EOF
    LIMIT=1
    ADD=2
    ALL=3
    ALTER=4
    AND=5
    ANY=6
    AS=7
    ASC=8
    AUTHORIZATION=9
    BACKUP=10
    BEGIN=11
    BETWEEN=12
    BREAK=13
    BROWSE=14
    BULK=15
    BY=16
    CASCADE=17
    CASE=18
    CHECK=19
    CHECKPOINT=20
    CLOSE=21
    CLUSTERED=22
    COALESCE=23
    COLLATE=24
    COLUMN=25
    COMMIT=26
    COMPUTE=27
    CONSTRAINT=28
    CONTAINS=29
    CONTAINSTABLE=30
    CONTINUE=31
    CONVERT=32
    CREATE=33
    CROSS=34
    CURRENT=35
    CURRENT_DATE=36
    CURRENT_TIME=37
    CURRENT_TIMESTAMP=38
    CURRENT_USER=39
    CURSOR=40
    DATABASE=41
    DBCC=42
    DEALLOCATE=43
    DECLARE=44
    DEFAULT=45
    DELETE=46
    DENY=47
    DESC=48
    DISK=49
    DISTINCT=50
    DISTRIBUTED=51
    DOUBLE=52
    DROP=53
    DUMP=54
    ELSE=55
    END=56
    ERRLVL=57
    ESCAPE=58
    EXCEPT=59
    EXEC=60
    EXECUTE=61
    EXISTS=62
    EXIT=63
    EXTERNAL=64
    FETCH=65
    FILE=66
    FILLFACTOR=67
    FOR=68
    FOREIGN=69
    FREETEXT=70
    FREETEXTTABLE=71
    FROM=72
    FULL=73
    FUNCTION=74
    GOTO=75
    GRANT=76
    GROUP=77
    HAVING=78
    HOLDLOCK=79
    IDENTITY=80
    IDENTITYCOL=81
    IDENTITY_INSERT=82
    IF=83
    IN=84
    INDEX=85
    INNER=86
    INSERT=87
    INTERSECT=88
    INTO=89
    IS=90
    JOIN=91
    KEY=92
    KILL=93
    LEFT=94
    LIKE=95
    LINENO=96
    LOAD=97
    MERGE=98
    NATIONAL=99
    NOCHECK=100
    NONCLUSTERED=101
    NOT=102
    NULL=103
    NULLIF=104
    OF=105
    OFF=106
    OFFSETS=107
    ON=108
    OPEN=109
    OPENDATASOURCE=110
    OPENQUERY=111
    OPENROWSET=112
    OPENXML=113
    OPTION=114
    OR=115
    ORDER=116
    OUTER=117
    OVER=118
    PERCENT=119
    PIVOT=120
    PLAN=121
    PRECISION=122
    PRIMARY=123
    PRINT=124
    PROC=125
    PROCEDURE=126
    PUBLIC=127
    RAISERROR=128
    READ=129
    READTEXT=130
    RECONFIGURE=131
    REFERENCES=132
    REPLICATION=133
    RESTORE=134
    RESTRICT=135
    RETURN=136
    REVERT=137
    REVOKE=138
    RIGHT=139
    ROLLBACK=140
    ROWCOUNT=141
    ROWGUIDCOL=142
    RULE=143
    SAVE=144
    SCHEMA=145
    SECURITYAUDIT=146
    SELECT=147
    SEMANTICKEYPHRASETABLE=148
    SEMANTICSIMILARITYDETAILSTABLE=149
    SEMANTICSIMILARITYTABLE=150
    SESSION_USER=151
    SET=152
    SETUSER=153
    SHUTDOWN=154
    SOME=155
    STATISTICS=156
    SYSTEM_USER=157
    TABLE=158
    TABLESAMPLE=159
    TEXTSIZE=160
    THEN=161
    TO=162
    TOP=163
    TRAN=164
    TRANSACTION=165
    TRIGGER=166
    TRUNCATE=167
    TRY_CONVERT=168
    TSEQUAL=169
    UNION=170
    UNIQUE=171
    UNPIVOT=172
    UPDATE=173
    UPDATETEXT=174
    USE=175
    USER=176
    VALUES=177
    VARYING=178
    VIEW=179
    WAITFOR=180
    WHEN=181
    WHERE=182
    WHILE=183
    WITH=184
    WITHIN=185
    WRITETEXT=186
    ABSOLUTE=187
    APPLY=188
    AUTO=189
    AVG=190
    BASE64=191
    CALLER=192
    CAST=193
    CATCH=194
    CHECKSUM_AGG=195
    COMMITTED=196
    CONCAT=197
    COOKIE=198
    COUNT=199
    COUNT_BIG=200
    DELAY=201
    DELETED=202
    DENSE_RANK=203
    DISABLE=204
    DYNAMIC=205
    ENCRYPTION=206
    FAST=207
    FAST_FORWARD=208
    FIRST=209
    FOLLOWING=210
    FORWARD_ONLY=211
    FULLSCAN=212
    GLOBAL=213
    GO=214
    GROUPING=215
    GROUPING_ID=216
    HASH=217
    INSENSITIVE=218
    INSERTED=219
    ISOLATION=220
    KEEPFIXED=221
    KEYSET=222
    LAST=223
    LEVEL=224
    LOCAL=225
    LOCK_ESCALATION=226
    LOGIN=227
    LOOP=228
    MARK=229
    MAX=230
    MIN=231
    MODIFY=232
    NEXT=233
    NAME=234
    NOCOUNT=235
    NOEXPAND=236
    NORECOMPUTE=237
    NTILE=238
    NUMBER=239
    OFFSET=240
    ONLY=241
    OPTIMISTIC=242
    OPTIMIZE=243
    OUT=244
    OUTPUT=245
    OWNER=246
    PARTITION=247
    PATH=248
    PRECEDING=249
    PRIOR=250
    RANGE=251
    RANK=252
    READONLY=253
    READ_ONLY=254
    RECOMPILE=255
    RELATIVE=256
    REMOTE=257
    REPEATABLE=258
    ROOT=259
    ROW=260
    ROWGUID=261
    ROWS=262
    ROW_NUMBER=263
    SAMPLE=264
    SCHEMABINDING=265
    SCROLL=266
    SCROLL_LOCKS=267
    SELF=268
    SERIALIZABLE=269
    SNAPSHOT=270
    STATIC=271
    STATS_STREAM=272
    STDEV=273
    STDEVP=274
    SUM=275
    THROW=276
    TIES=277
    TIME=278
    TRY=279
    TYPE=280
    TYPE_WARNING=281
    UNBOUNDED=282
    UNCOMMITTED=283
    UNKNOWN=284
    USING=285
    VAR=286
    VARP=287
    VIEW_METADATA=288
    WORK=289
    XML=290
    XMLNAMESPACES=291
    DOLLAR_ACTION=292
    SPACE=293
    COMMENT=294
    LINE_COMMENT=295
    DOUBLE_QUOTE_ID=296
    SQUARE_BRACKET_ID=297
    LOCAL_ID=298
    DECIMAL=299
    ID=300
    STRING=301
    BINARY=302
    FLOAT=303
    REAL=304
    EQUAL=305
    GREATER=306
    LESS=307
    EXCLAMATION=308
    PLUS_ASSIGN=309
    MINUS_ASSIGN=310
    MULT_ASSIGN=311
    DIV_ASSIGN=312
    MOD_ASSIGN=313
    AND_ASSIGN=314
    XOR_ASSIGN=315
    OR_ASSIGN=316
    DOT=317
    UNDERLINE=318
    AT=319
    SHARP=320
    DOLLAR=321
    LR_BRACKET=322
    RR_BRACKET=323
    COMMA=324
    SEMI=325
    COLON=326
    STAR=327
    DIVIDE=328
    MODULE=329
    PLUS=330
    MINUS=331
    BIT_NOT=332
    BIT_OR=333
    BIT_AND=334
    BIT_XOR=335

    def __init__(self, input:TokenStream):
        super().__init__(input)
        self.checkVersion("4.5.1")
        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)
        self._predicates = None



    class RootContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def select_statement(self):
            return self.getTypedRuleContext(SparksqlParser.Select_statementContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_root

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRoot" ):
                listener.enterRoot(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRoot" ):
                listener.exitRoot(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRoot" ):
                return visitor.visitRoot(self)
            else:
                return visitor.visitChildren(self)




    def root(self):

        localctx = SparksqlParser.RootContext(self, self._ctx, self.state)
        self.enterRule(localctx, 0, self.RULE_root)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 96
            self.select_statement()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Select_statementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def query_expression(self):
            return self.getTypedRuleContext(SparksqlParser.Query_expressionContext,0)


        def with_expression(self):
            return self.getTypedRuleContext(SparksqlParser.With_expressionContext,0)


        def order_by_clause(self):
            return self.getTypedRuleContext(SparksqlParser.Order_by_clauseContext,0)


        def limit_clause(self):
            return self.getTypedRuleContext(SparksqlParser.Limit_clauseContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_select_statement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelect_statement" ):
                listener.enterSelect_statement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelect_statement" ):
                listener.exitSelect_statement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelect_statement" ):
                return visitor.visitSelect_statement(self)
            else:
                return visitor.visitChildren(self)




    def select_statement(self):

        localctx = SparksqlParser.Select_statementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 2, self.RULE_select_statement)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 99
            _la = self._input.LA(1)
            if _la==SparksqlParser.WITH:
                self.state = 98
                self.with_expression()


            self.state = 101
            self.query_expression()
            self.state = 103
            la_ = self._interp.adaptivePredict(self._input,1,self._ctx)
            if la_ == 1:
                self.state = 102
                self.order_by_clause()


            self.state = 106
            la_ = self._interp.adaptivePredict(self._input,2,self._ctx)
            if la_ == 1:
                self.state = 105
                self.limit_clause()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Limit_clauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def LIMIT(self):
            return self.getToken(SparksqlParser.LIMIT, 0)

        def number(self):
            return self.getTypedRuleContext(SparksqlParser.NumberContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_limit_clause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLimit_clause" ):
                listener.enterLimit_clause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLimit_clause" ):
                listener.exitLimit_clause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLimit_clause" ):
                return visitor.visitLimit_clause(self)
            else:
                return visitor.visitChildren(self)




    def limit_clause(self):

        localctx = SparksqlParser.Limit_clauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 4, self.RULE_limit_clause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 108
            self.match(SparksqlParser.LIMIT)
            self.state = 109
            self.number()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Order_by_clauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ORDER(self):
            return self.getToken(SparksqlParser.ORDER, 0)

        def BY(self):
            return self.getToken(SparksqlParser.BY, 0)

        def order_by_expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Order_by_expressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Order_by_expressionContext,i)


        def OFFSET(self):
            return self.getToken(SparksqlParser.OFFSET, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def ROW(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.ROW)
            else:
                return self.getToken(SparksqlParser.ROW, i)

        def ROWS(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.ROWS)
            else:
                return self.getToken(SparksqlParser.ROWS, i)

        def FETCH(self):
            return self.getToken(SparksqlParser.FETCH, 0)

        def ONLY(self):
            return self.getToken(SparksqlParser.ONLY, 0)

        def FIRST(self):
            return self.getToken(SparksqlParser.FIRST, 0)

        def NEXT(self):
            return self.getToken(SparksqlParser.NEXT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_order_by_clause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrder_by_clause" ):
                listener.enterOrder_by_clause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrder_by_clause" ):
                listener.exitOrder_by_clause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrder_by_clause" ):
                return visitor.visitOrder_by_clause(self)
            else:
                return visitor.visitChildren(self)




    def order_by_clause(self):

        localctx = SparksqlParser.Order_by_clauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 6, self.RULE_order_by_clause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 111
            self.match(SparksqlParser.ORDER)
            self.state = 112
            self.match(SparksqlParser.BY)
            self.state = 113
            self.order_by_expression()
            self.state = 118
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,3,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 114
                    self.match(SparksqlParser.COMMA)
                    self.state = 115
                    self.order_by_expression() 
                self.state = 120
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,3,self._ctx)

            self.state = 132
            la_ = self._interp.adaptivePredict(self._input,5,self._ctx)
            if la_ == 1:
                self.state = 121
                self.match(SparksqlParser.OFFSET)
                self.state = 122
                self.expression(0)
                self.state = 123
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ROW or _la==SparksqlParser.ROWS):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                self.state = 130
                _la = self._input.LA(1)
                if _la==SparksqlParser.FETCH:
                    self.state = 124
                    self.match(SparksqlParser.FETCH)
                    self.state = 125
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.FIRST or _la==SparksqlParser.NEXT):
                        self._errHandler.recoverInline(self)
                    else:
                        self.consume()
                    self.state = 126
                    self.expression(0)
                    self.state = 127
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.ROW or _la==SparksqlParser.ROWS):
                        self._errHandler.recoverInline(self)
                    else:
                        self.consume()
                    self.state = 128
                    self.match(SparksqlParser.ONLY)




        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Order_by_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def ASC(self):
            return self.getToken(SparksqlParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparksqlParser.DESC, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_order_by_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrder_by_expression" ):
                listener.enterOrder_by_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrder_by_expression" ):
                listener.exitOrder_by_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrder_by_expression" ):
                return visitor.visitOrder_by_expression(self)
            else:
                return visitor.visitChildren(self)




    def order_by_expression(self):

        localctx = SparksqlParser.Order_by_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 8, self.RULE_order_by_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 134
            self.expression(0)
            self.state = 136
            _la = self._input.LA(1)
            if _la==SparksqlParser.ASC or _la==SparksqlParser.DESC:
                self.state = 135
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ASC or _la==SparksqlParser.DESC):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Query_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def query_specification(self):
            return self.getTypedRuleContext(SparksqlParser.Query_specificationContext,0)


        def query_expression(self):
            return self.getTypedRuleContext(SparksqlParser.Query_expressionContext,0)


        def union(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.UnionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.UnionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_query_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuery_expression" ):
                listener.enterQuery_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuery_expression" ):
                listener.exitQuery_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuery_expression" ):
                return visitor.visitQuery_expression(self)
            else:
                return visitor.visitChildren(self)




    def query_expression(self):

        localctx = SparksqlParser.Query_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 10, self.RULE_query_expression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 143
            token = self._input.LA(1)
            if token in [SparksqlParser.SELECT]:
                self.state = 138
                self.query_specification()

            elif token in [SparksqlParser.LR_BRACKET]:
                self.state = 139
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 140
                self.query_expression()
                self.state = 141
                self.match(SparksqlParser.RR_BRACKET)

            else:
                raise NoViableAltException(self)

            self.state = 148
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,8,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 145
                    self.union() 
                self.state = 150
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,8,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class With_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WITH(self):
            return self.getToken(SparksqlParser.WITH, 0)

        def common_table_expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Common_table_expressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Common_table_expressionContext,i)


        def XMLNAMESPACES(self):
            return self.getToken(SparksqlParser.XMLNAMESPACES, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_with_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWith_expression" ):
                listener.enterWith_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWith_expression" ):
                listener.exitWith_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWith_expression" ):
                return visitor.visitWith_expression(self)
            else:
                return visitor.visitChildren(self)




    def with_expression(self):

        localctx = SparksqlParser.With_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 12, self.RULE_with_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 151
            self.match(SparksqlParser.WITH)
            self.state = 154
            la_ = self._interp.adaptivePredict(self._input,9,self._ctx)
            if la_ == 1:
                self.state = 152
                self.match(SparksqlParser.XMLNAMESPACES)
                self.state = 153
                self.match(SparksqlParser.COMMA)


            self.state = 156
            self.common_table_expression()
            self.state = 161
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 157
                self.match(SparksqlParser.COMMA)
                self.state = 158
                self.common_table_expression()
                self.state = 163
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Common_table_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.expression_name = None # Id_1Context

        def AS(self):
            return self.getToken(SparksqlParser.AS, 0)

        def select_statement(self):
            return self.getTypedRuleContext(SparksqlParser.Select_statementContext,0)


        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def column_name_list(self):
            return self.getTypedRuleContext(SparksqlParser.Column_name_listContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_common_table_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCommon_table_expression" ):
                listener.enterCommon_table_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCommon_table_expression" ):
                listener.exitCommon_table_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCommon_table_expression" ):
                return visitor.visitCommon_table_expression(self)
            else:
                return visitor.visitChildren(self)




    def common_table_expression(self):

        localctx = SparksqlParser.Common_table_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 14, self.RULE_common_table_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 164
            localctx.expression_name = self.id_1()
            self.state = 169
            _la = self._input.LA(1)
            if _la==SparksqlParser.LR_BRACKET:
                self.state = 165
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 166
                self.column_name_list()
                self.state = 167
                self.match(SparksqlParser.RR_BRACKET)


            self.state = 171
            self.match(SparksqlParser.AS)
            self.state = 172
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 173
            self.select_statement()
            self.state = 174
            self.match(SparksqlParser.RR_BRACKET)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_name_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def column_name(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Column_nameContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Column_nameContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_column_name_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_name_list" ):
                listener.enterColumn_name_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_name_list" ):
                listener.exitColumn_name_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_name_list" ):
                return visitor.visitColumn_name_list(self)
            else:
                return visitor.visitChildren(self)




    def column_name_list(self):

        localctx = SparksqlParser.Column_name_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 16, self.RULE_column_name_list)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 176
            self.column_name()
            self.state = 181
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 177
                self.match(SparksqlParser.COMMA)
                self.state = 178
                self.column_name()
                self.state = 183
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Query_specificationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.into_table = None # Table_nameContext
            self.where = None # Search_conditionContext
            self.having = None # Search_conditionContext

        def SELECT(self):
            return self.getToken(SparksqlParser.SELECT, 0)

        def select_list(self):
            return self.getTypedRuleContext(SparksqlParser.Select_listContext,0)


        def TOP(self):
            return self.getToken(SparksqlParser.TOP, 0)

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def INTO(self):
            return self.getToken(SparksqlParser.INTO, 0)

        def FROM(self):
            return self.getToken(SparksqlParser.FROM, 0)

        def table_source(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Table_sourceContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Table_sourceContext,i)


        def WHERE(self):
            return self.getToken(SparksqlParser.WHERE, 0)

        def GROUP(self):
            return self.getToken(SparksqlParser.GROUP, 0)

        def BY(self):
            return self.getToken(SparksqlParser.BY, 0)

        def group_by_item(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Group_by_itemContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Group_by_itemContext,i)


        def HAVING(self):
            return self.getToken(SparksqlParser.HAVING, 0)

        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def DISTINCT(self):
            return self.getToken(SparksqlParser.DISTINCT, 0)

        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def search_condition(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_conditionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,i)


        def PERCENT(self):
            return self.getToken(SparksqlParser.PERCENT, 0)

        def WITH(self):
            return self.getToken(SparksqlParser.WITH, 0)

        def TIES(self):
            return self.getToken(SparksqlParser.TIES, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_query_specification

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuery_specification" ):
                listener.enterQuery_specification(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuery_specification" ):
                listener.exitQuery_specification(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuery_specification" ):
                return visitor.visitQuery_specification(self)
            else:
                return visitor.visitChildren(self)




    def query_specification(self):

        localctx = SparksqlParser.Query_specificationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 18, self.RULE_query_specification)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 184
            self.match(SparksqlParser.SELECT)
            self.state = 186
            _la = self._input.LA(1)
            if _la==SparksqlParser.ALL or _la==SparksqlParser.DISTINCT:
                self.state = 185
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ALL or _la==SparksqlParser.DISTINCT):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()


            self.state = 197
            _la = self._input.LA(1)
            if _la==SparksqlParser.TOP:
                self.state = 188
                self.match(SparksqlParser.TOP)
                self.state = 189
                self.expression(0)
                self.state = 191
                _la = self._input.LA(1)
                if _la==SparksqlParser.PERCENT:
                    self.state = 190
                    self.match(SparksqlParser.PERCENT)


                self.state = 195
                _la = self._input.LA(1)
                if _la==SparksqlParser.WITH:
                    self.state = 193
                    self.match(SparksqlParser.WITH)
                    self.state = 194
                    self.match(SparksqlParser.TIES)




            self.state = 199
            self.select_list()
            self.state = 202
            _la = self._input.LA(1)
            if _la==SparksqlParser.INTO:
                self.state = 200
                self.match(SparksqlParser.INTO)
                self.state = 201
                localctx.into_table = self.table_name()


            self.state = 213
            _la = self._input.LA(1)
            if _la==SparksqlParser.FROM:
                self.state = 204
                self.match(SparksqlParser.FROM)
                self.state = 205
                self.table_source()
                self.state = 210
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,18,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 206
                        self.match(SparksqlParser.COMMA)
                        self.state = 207
                        self.table_source() 
                    self.state = 212
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,18,self._ctx)



            self.state = 217
            la_ = self._interp.adaptivePredict(self._input,20,self._ctx)
            if la_ == 1:
                self.state = 215
                self.match(SparksqlParser.WHERE)
                self.state = 216
                localctx.where = self.search_condition()


            self.state = 229
            la_ = self._interp.adaptivePredict(self._input,22,self._ctx)
            if la_ == 1:
                self.state = 219
                self.match(SparksqlParser.GROUP)
                self.state = 220
                self.match(SparksqlParser.BY)
                self.state = 221
                self.group_by_item()
                self.state = 226
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,21,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 222
                        self.match(SparksqlParser.COMMA)
                        self.state = 223
                        self.group_by_item() 
                    self.state = 228
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,21,self._ctx)



            self.state = 233
            la_ = self._interp.adaptivePredict(self._input,23,self._ctx)
            if la_ == 1:
                self.state = 231
                self.match(SparksqlParser.HAVING)
                self.state = 232
                localctx.having = self.search_condition()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class UnionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def UNION(self):
            return self.getToken(SparksqlParser.UNION, 0)

        def EXCEPT(self):
            return self.getToken(SparksqlParser.EXCEPT, 0)

        def INTERSECT(self):
            return self.getToken(SparksqlParser.INTERSECT, 0)

        def query_specification(self):
            return self.getTypedRuleContext(SparksqlParser.Query_specificationContext,0)


        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def query_expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Query_expressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Query_expressionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_union

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnion" ):
                listener.enterUnion(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnion" ):
                listener.exitUnion(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnion" ):
                return visitor.visitUnion(self)
            else:
                return visitor.visitChildren(self)




    def union(self):

        localctx = SparksqlParser.UnionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 20, self.RULE_union)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 241
            token = self._input.LA(1)
            if token in [SparksqlParser.UNION]:
                self.state = 235
                self.match(SparksqlParser.UNION)
                self.state = 237
                _la = self._input.LA(1)
                if _la==SparksqlParser.ALL:
                    self.state = 236
                    self.match(SparksqlParser.ALL)



            elif token in [SparksqlParser.EXCEPT]:
                self.state = 239
                self.match(SparksqlParser.EXCEPT)

            elif token in [SparksqlParser.INTERSECT]:
                self.state = 240
                self.match(SparksqlParser.INTERSECT)

            else:
                raise NoViableAltException(self)

            self.state = 252
            token = self._input.LA(1)
            if token in [SparksqlParser.SELECT]:
                self.state = 243
                self.query_specification()

            elif token in [SparksqlParser.LR_BRACKET]:
                self.state = 248 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 244
                    self.match(SparksqlParser.LR_BRACKET)
                    self.state = 245
                    self.query_expression()
                    self.state = 246
                    self.match(SparksqlParser.RR_BRACKET)
                    self.state = 250 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparksqlParser.LR_BRACKET):
                        break


            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_sourceContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_source_item_joined(self):
            return self.getTypedRuleContext(SparksqlParser.Table_source_item_joinedContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_source

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_source" ):
                listener.enterTable_source(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_source" ):
                listener.exitTable_source(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_source" ):
                return visitor.visitTable_source(self)
            else:
                return visitor.visitChildren(self)




    def table_source(self):

        localctx = SparksqlParser.Table_sourceContext(self, self._ctx, self.state)
        self.enterRule(localctx, 22, self.RULE_table_source)
        try:
            self.state = 259
            la_ = self._interp.adaptivePredict(self._input,28,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 254
                self.table_source_item_joined()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 255
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 256
                self.table_source_item_joined()
                self.state = 257
                self.match(SparksqlParser.RR_BRACKET)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_source_item_joinedContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_source_item(self):
            return self.getTypedRuleContext(SparksqlParser.Table_source_itemContext,0)


        def join_part(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Join_partContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Join_partContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_source_item_joined

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_source_item_joined" ):
                listener.enterTable_source_item_joined(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_source_item_joined" ):
                listener.exitTable_source_item_joined(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_source_item_joined" ):
                return visitor.visitTable_source_item_joined(self)
            else:
                return visitor.visitChildren(self)




    def table_source_item_joined(self):

        localctx = SparksqlParser.Table_source_item_joinedContext(self, self._ctx, self.state)
        self.enterRule(localctx, 24, self.RULE_table_source_item_joined)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 261
            self.table_source_item()
            self.state = 265
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,29,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 262
                    self.join_part() 
                self.state = 267
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,29,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_source_itemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_name_with_hint(self):
            return self.getTypedRuleContext(SparksqlParser.Table_name_with_hintContext,0)


        def as_table_alias(self):
            return self.getTypedRuleContext(SparksqlParser.As_table_aliasContext,0)


        def derived_table(self):
            return self.getTypedRuleContext(SparksqlParser.Derived_tableContext,0)


        def column_alias_list(self):
            return self.getTypedRuleContext(SparksqlParser.Column_alias_listContext,0)


        def LOCAL_ID(self):
            return self.getToken(SparksqlParser.LOCAL_ID, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_table_source_item

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_source_item" ):
                listener.enterTable_source_item(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_source_item" ):
                listener.exitTable_source_item(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_source_item" ):
                return visitor.visitTable_source_item(self)
            else:
                return visitor.visitChildren(self)




    def table_source_item(self):

        localctx = SparksqlParser.Table_source_itemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 26, self.RULE_table_source_item)
        self._la = 0 # Token type
        try:
            self.state = 283
            token = self._input.LA(1)
            if token in [SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.DOUBLE_QUOTE_ID, SparksqlParser.SQUARE_BRACKET_ID, SparksqlParser.ID, SparksqlParser.DOT]:
                self.enterOuterAlt(localctx, 1)
                self.state = 268
                self.table_name_with_hint()
                self.state = 270
                la_ = self._interp.adaptivePredict(self._input,30,self._ctx)
                if la_ == 1:
                    self.state = 269
                    self.as_table_alias()



            elif token in [SparksqlParser.SELECT, SparksqlParser.VALUES, SparksqlParser.WITH, SparksqlParser.LR_BRACKET]:
                self.enterOuterAlt(localctx, 2)
                self.state = 272
                self.derived_table()
                self.state = 277
                la_ = self._interp.adaptivePredict(self._input,32,self._ctx)
                if la_ == 1:
                    self.state = 273
                    self.as_table_alias()
                    self.state = 275
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.LR_BRACKET:
                        self.state = 274
                        self.column_alias_list()





            elif token in [SparksqlParser.LOCAL_ID]:
                self.enterOuterAlt(localctx, 3)
                self.state = 279
                self.match(SparksqlParser.LOCAL_ID)
                self.state = 281
                la_ = self._interp.adaptivePredict(self._input,33,self._ctx)
                if la_ == 1:
                    self.state = 280
                    self.as_table_alias()



            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Group_by_itemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_group_by_item

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterGroup_by_item" ):
                listener.enterGroup_by_item(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitGroup_by_item" ):
                listener.exitGroup_by_item(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitGroup_by_item" ):
                return visitor.visitGroup_by_item(self)
            else:
                return visitor.visitChildren(self)




    def group_by_item(self):

        localctx = SparksqlParser.Group_by_itemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 28, self.RULE_group_by_item)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 285
            self.expression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Derived_tableContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def subquery(self):
            return self.getTypedRuleContext(SparksqlParser.SubqueryContext,0)


        def table_value_constructor(self):
            return self.getTypedRuleContext(SparksqlParser.Table_value_constructorContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_derived_table

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDerived_table" ):
                listener.enterDerived_table(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDerived_table" ):
                listener.exitDerived_table(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDerived_table" ):
                return visitor.visitDerived_table(self)
            else:
                return visitor.visitChildren(self)




    def derived_table(self):

        localctx = SparksqlParser.Derived_tableContext(self, self._ctx, self.state)
        self.enterRule(localctx, 30, self.RULE_derived_table)
        try:
            self.state = 293
            la_ = self._interp.adaptivePredict(self._input,35,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 287
                self.subquery()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 288
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 289
                self.subquery()
                self.state = 290
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 292
                self.table_value_constructor()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_value_constructorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def VALUES(self):
            return self.getToken(SparksqlParser.VALUES, 0)

        def expression_list(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Expression_listContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Expression_listContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_value_constructor

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_value_constructor" ):
                listener.enterTable_value_constructor(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_value_constructor" ):
                listener.exitTable_value_constructor(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_value_constructor" ):
                return visitor.visitTable_value_constructor(self)
            else:
                return visitor.visitChildren(self)




    def table_value_constructor(self):

        localctx = SparksqlParser.Table_value_constructorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 32, self.RULE_table_value_constructor)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 295
            self.match(SparksqlParser.VALUES)
            self.state = 296
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 297
            self.expression_list()
            self.state = 298
            self.match(SparksqlParser.RR_BRACKET)
            self.state = 306
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,36,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 299
                    self.match(SparksqlParser.COMMA)
                    self.state = 300
                    self.match(SparksqlParser.LR_BRACKET)
                    self.state = 301
                    self.expression_list()
                    self.state = 302
                    self.match(SparksqlParser.RR_BRACKET) 
                self.state = 308
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,36,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Expression_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_expression_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExpression_list" ):
                listener.enterExpression_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExpression_list" ):
                listener.exitExpression_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExpression_list" ):
                return visitor.visitExpression_list(self)
            else:
                return visitor.visitChildren(self)




    def expression_list(self):

        localctx = SparksqlParser.Expression_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 34, self.RULE_expression_list)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 309
            self.expression(0)
            self.state = 314
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 310
                self.match(SparksqlParser.COMMA)
                self.state = 311
                self.expression(0)
                self.state = 316
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SubqueryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def select_statement(self):
            return self.getTypedRuleContext(SparksqlParser.Select_statementContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_subquery

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubquery" ):
                listener.enterSubquery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubquery" ):
                listener.exitSubquery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubquery" ):
                return visitor.visitSubquery(self)
            else:
                return visitor.visitChildren(self)




    def subquery(self):

        localctx = SparksqlParser.SubqueryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 36, self.RULE_subquery)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 317
            self.select_statement()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Join_partContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.join_type = None # Token
            self.join_hint = None # Token

        def JOIN(self):
            return self.getToken(SparksqlParser.JOIN, 0)

        def table_source(self):
            return self.getTypedRuleContext(SparksqlParser.Table_sourceContext,0)


        def ON(self):
            return self.getToken(SparksqlParser.ON, 0)

        def search_condition(self):
            return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,0)


        def LEFT(self):
            return self.getToken(SparksqlParser.LEFT, 0)

        def RIGHT(self):
            return self.getToken(SparksqlParser.RIGHT, 0)

        def FULL(self):
            return self.getToken(SparksqlParser.FULL, 0)

        def INNER(self):
            return self.getToken(SparksqlParser.INNER, 0)

        def OUTER(self):
            return self.getToken(SparksqlParser.OUTER, 0)

        def LOOP(self):
            return self.getToken(SparksqlParser.LOOP, 0)

        def HASH(self):
            return self.getToken(SparksqlParser.HASH, 0)

        def MERGE(self):
            return self.getToken(SparksqlParser.MERGE, 0)

        def REMOTE(self):
            return self.getToken(SparksqlParser.REMOTE, 0)

        def CROSS(self):
            return self.getToken(SparksqlParser.CROSS, 0)

        def APPLY(self):
            return self.getToken(SparksqlParser.APPLY, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_join_part

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoin_part" ):
                listener.enterJoin_part(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoin_part" ):
                listener.exitJoin_part(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoin_part" ):
                return visitor.visitJoin_part(self)
            else:
                return visitor.visitChildren(self)




    def join_part(self):

        localctx = SparksqlParser.Join_partContext(self, self._ctx, self.state)
        self.enterRule(localctx, 38, self.RULE_join_part)
        self._la = 0 # Token type
        try:
            self.state = 345
            la_ = self._interp.adaptivePredict(self._input,42,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 326
                token = self._input.LA(1)
                if token in [SparksqlParser.INNER, SparksqlParser.JOIN, SparksqlParser.MERGE, SparksqlParser.HASH, SparksqlParser.LOOP, SparksqlParser.REMOTE]:
                    self.state = 320
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.INNER:
                        self.state = 319
                        self.match(SparksqlParser.INNER)



                elif token in [SparksqlParser.FULL, SparksqlParser.LEFT, SparksqlParser.RIGHT]:
                    self.state = 322
                    localctx.join_type = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.FULL or _la==SparksqlParser.LEFT or _la==SparksqlParser.RIGHT):
                        localctx.join_type = self._errHandler.recoverInline(self)
                    else:
                        self.consume()
                    self.state = 324
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.OUTER:
                        self.state = 323
                        self.match(SparksqlParser.OUTER)



                else:
                    raise NoViableAltException(self)

                self.state = 329
                _la = self._input.LA(1)
                if _la==SparksqlParser.MERGE or ((((_la - 217)) & ~0x3f) == 0 and ((1 << (_la - 217)) & ((1 << (SparksqlParser.HASH - 217)) | (1 << (SparksqlParser.LOOP - 217)) | (1 << (SparksqlParser.REMOTE - 217)))) != 0):
                    self.state = 328
                    localctx.join_hint = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.MERGE or ((((_la - 217)) & ~0x3f) == 0 and ((1 << (_la - 217)) & ((1 << (SparksqlParser.HASH - 217)) | (1 << (SparksqlParser.LOOP - 217)) | (1 << (SparksqlParser.REMOTE - 217)))) != 0)):
                        localctx.join_hint = self._errHandler.recoverInline(self)
                    else:
                        self.consume()


                self.state = 331
                self.match(SparksqlParser.JOIN)
                self.state = 332
                self.table_source()
                self.state = 333
                self.match(SparksqlParser.ON)
                self.state = 334
                self.search_condition()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 336
                self.match(SparksqlParser.CROSS)
                self.state = 337
                self.match(SparksqlParser.JOIN)
                self.state = 338
                self.table_source()
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 339
                self.match(SparksqlParser.CROSS)
                self.state = 340
                self.match(SparksqlParser.APPLY)
                self.state = 341
                self.table_source()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 342
                self.match(SparksqlParser.OUTER)
                self.state = 343
                self.match(SparksqlParser.APPLY)
                self.state = 344
                self.table_source()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_name_with_hintContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def with_table_hints(self):
            return self.getTypedRuleContext(SparksqlParser.With_table_hintsContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_name_with_hint

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_name_with_hint" ):
                listener.enterTable_name_with_hint(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_name_with_hint" ):
                listener.exitTable_name_with_hint(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_name_with_hint" ):
                return visitor.visitTable_name_with_hint(self)
            else:
                return visitor.visitChildren(self)




    def table_name_with_hint(self):

        localctx = SparksqlParser.Table_name_with_hintContext(self, self._ctx, self.state)
        self.enterRule(localctx, 40, self.RULE_table_name_with_hint)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 347
            self.table_name()
            self.state = 349
            _la = self._input.LA(1)
            if _la==SparksqlParser.WITH or _la==SparksqlParser.LR_BRACKET:
                self.state = 348
                self.with_table_hints()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class As_table_aliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_alias(self):
            return self.getTypedRuleContext(SparksqlParser.Table_aliasContext,0)


        def AS(self):
            return self.getToken(SparksqlParser.AS, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_as_table_alias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAs_table_alias" ):
                listener.enterAs_table_alias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAs_table_alias" ):
                listener.exitAs_table_alias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAs_table_alias" ):
                return visitor.visitAs_table_alias(self)
            else:
                return visitor.visitChildren(self)




    def as_table_alias(self):

        localctx = SparksqlParser.As_table_aliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 42, self.RULE_as_table_alias)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 352
            _la = self._input.LA(1)
            if _la==SparksqlParser.AS:
                self.state = 351
                self.match(SparksqlParser.AS)


            self.state = 354
            self.table_alias()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_aliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def with_table_hints(self):
            return self.getTypedRuleContext(SparksqlParser.With_table_hintsContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_alias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_alias" ):
                listener.enterTable_alias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_alias" ):
                listener.exitTable_alias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_alias" ):
                return visitor.visitTable_alias(self)
            else:
                return visitor.visitChildren(self)




    def table_alias(self):

        localctx = SparksqlParser.Table_aliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 44, self.RULE_table_alias)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 356
            self.id_1()
            self.state = 358
            la_ = self._interp.adaptivePredict(self._input,45,self._ctx)
            if la_ == 1:
                self.state = 357
                self.with_table_hints()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class With_table_hintsContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_hint(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Table_hintContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Table_hintContext,i)


        def WITH(self):
            return self.getToken(SparksqlParser.WITH, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_with_table_hints

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWith_table_hints" ):
                listener.enterWith_table_hints(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWith_table_hints" ):
                listener.exitWith_table_hints(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWith_table_hints" ):
                return visitor.visitWith_table_hints(self)
            else:
                return visitor.visitChildren(self)




    def with_table_hints(self):

        localctx = SparksqlParser.With_table_hintsContext(self, self._ctx, self.state)
        self.enterRule(localctx, 46, self.RULE_with_table_hints)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 361
            _la = self._input.LA(1)
            if _la==SparksqlParser.WITH:
                self.state = 360
                self.match(SparksqlParser.WITH)


            self.state = 363
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 364
            self.table_hint()
            self.state = 369
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 365
                self.match(SparksqlParser.COMMA)
                self.state = 366
                self.table_hint()
                self.state = 371
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 372
            self.match(SparksqlParser.RR_BRACKET)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_hintContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INDEX(self):
            return self.getToken(SparksqlParser.INDEX, 0)

        def index_value(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Index_valueContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Index_valueContext,i)


        def ID(self):
            return self.getToken(SparksqlParser.ID, 0)

        def NOEXPAND(self):
            return self.getToken(SparksqlParser.NOEXPAND, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_table_hint

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_hint" ):
                listener.enterTable_hint(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_hint" ):
                listener.exitTable_hint(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_hint" ):
                return visitor.visitTable_hint(self)
            else:
                return visitor.visitChildren(self)




    def table_hint(self):

        localctx = SparksqlParser.Table_hintContext(self, self._ctx, self.state)
        self.enterRule(localctx, 48, self.RULE_table_hint)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 375
            _la = self._input.LA(1)
            if _la==SparksqlParser.NOEXPAND:
                self.state = 374
                self.match(SparksqlParser.NOEXPAND)


            self.state = 393
            la_ = self._interp.adaptivePredict(self._input,50,self._ctx)
            if la_ == 1:
                self.state = 377
                self.match(SparksqlParser.INDEX)
                self.state = 378
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 379
                self.index_value()
                self.state = 384
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparksqlParser.COMMA:
                    self.state = 380
                    self.match(SparksqlParser.COMMA)
                    self.state = 381
                    self.index_value()
                    self.state = 386
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 387
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 2:
                self.state = 389
                self.match(SparksqlParser.INDEX)
                self.state = 390
                self.match(SparksqlParser.EQUAL)
                self.state = 391
                self.index_value()
                pass

            elif la_ == 3:
                self.state = 392
                self.match(SparksqlParser.ID)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Index_valueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ID(self):
            return self.getToken(SparksqlParser.ID, 0)

        def DECIMAL(self):
            return self.getToken(SparksqlParser.DECIMAL, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_index_value

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIndex_value" ):
                listener.enterIndex_value(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIndex_value" ):
                listener.exitIndex_value(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIndex_value" ):
                return visitor.visitIndex_value(self)
            else:
                return visitor.visitChildren(self)




    def index_value(self):

        localctx = SparksqlParser.Index_valueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 50, self.RULE_index_value)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 395
            _la = self._input.LA(1)
            if not(_la==SparksqlParser.DECIMAL or _la==SparksqlParser.ID):
                self._errHandler.recoverInline(self)
            else:
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_alias_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def column_alias(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Column_aliasContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Column_aliasContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_column_alias_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_alias_list" ):
                listener.enterColumn_alias_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_alias_list" ):
                listener.exitColumn_alias_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_alias_list" ):
                return visitor.visitColumn_alias_list(self)
            else:
                return visitor.visitChildren(self)




    def column_alias_list(self):

        localctx = SparksqlParser.Column_alias_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 52, self.RULE_column_alias_list)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 397
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 398
            self.column_alias()
            self.state = 403
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 399
                self.match(SparksqlParser.COMMA)
                self.state = 400
                self.column_alias()
                self.state = 405
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 406
            self.match(SparksqlParser.RR_BRACKET)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_aliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def STRING(self):
            return self.getToken(SparksqlParser.STRING, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_column_alias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_alias" ):
                listener.enterColumn_alias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_alias" ):
                listener.exitColumn_alias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_alias" ):
                return visitor.visitColumn_alias(self)
            else:
                return visitor.visitChildren(self)




    def column_alias(self):

        localctx = SparksqlParser.Column_aliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 54, self.RULE_column_alias)
        try:
            self.state = 410
            token = self._input.LA(1)
            if token in [SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.DOUBLE_QUOTE_ID, SparksqlParser.SQUARE_BRACKET_ID, SparksqlParser.ID]:
                self.enterOuterAlt(localctx, 1)
                self.state = 408
                self.id_1()

            elif token in [SparksqlParser.STRING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 409
                self.match(SparksqlParser.STRING)

            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.op = None # Token

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def DEFAULT(self):
            return self.getToken(SparksqlParser.DEFAULT, 0)

        def NULL(self):
            return self.getToken(SparksqlParser.NULL, 0)

        def LOCAL_ID(self):
            return self.getToken(SparksqlParser.LOCAL_ID, 0)

        def constant(self):
            return self.getTypedRuleContext(SparksqlParser.ConstantContext,0)


        def case_expr(self):
            return self.getTypedRuleContext(SparksqlParser.Case_exprContext,0)


        def full_column_name(self):
            return self.getTypedRuleContext(SparksqlParser.Full_column_nameContext,0)


        def subquery(self):
            return self.getTypedRuleContext(SparksqlParser.SubqueryContext,0)


        def aggregate_windowed_function(self):
            return self.getTypedRuleContext(SparksqlParser.Aggregate_windowed_functionContext,0)


        def comparison_operator(self):
            return self.getTypedRuleContext(SparksqlParser.Comparison_operatorContext,0)


        def COLLATE(self):
            return self.getToken(SparksqlParser.COLLATE, 0)

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExpression" ):
                listener.enterExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExpression" ):
                listener.exitExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExpression" ):
                return visitor.visitExpression(self)
            else:
                return visitor.visitChildren(self)



    def expression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparksqlParser.ExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 56
        self.enterRecursionRule(localctx, 56, self.RULE_expression, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 432
            la_ = self._interp.adaptivePredict(self._input,53,self._ctx)
            if la_ == 1:
                self.state = 413
                self.match(SparksqlParser.BIT_NOT)
                self.state = 414
                self.expression(6)
                pass

            elif la_ == 2:
                self.state = 415
                localctx.op = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS):
                    localctx.op = self._errHandler.recoverInline(self)
                else:
                    self.consume()
                self.state = 416
                self.expression(4)
                pass

            elif la_ == 3:
                self.state = 417
                self.match(SparksqlParser.DEFAULT)
                pass

            elif la_ == 4:
                self.state = 418
                self.match(SparksqlParser.NULL)
                pass

            elif la_ == 5:
                self.state = 419
                self.match(SparksqlParser.LOCAL_ID)
                pass

            elif la_ == 6:
                self.state = 420
                self.constant()
                pass

            elif la_ == 7:
                self.state = 421
                self.case_expr()
                pass

            elif la_ == 8:
                self.state = 422
                self.full_column_name()
                pass

            elif la_ == 9:
                self.state = 423
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 424
                self.expression(0)
                self.state = 425
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 10:
                self.state = 427
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 428
                self.subquery()
                self.state = 429
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 11:
                self.state = 431
                self.aggregate_windowed_function()
                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 449
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,55,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 447
                    la_ = self._interp.adaptivePredict(self._input,54,self._ctx)
                    if la_ == 1:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 434
                        if not self.precpred(self._ctx, 5):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 5)")
                        self.state = 435
                        localctx.op = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 327)) & ~0x3f) == 0 and ((1 << (_la - 327)) & ((1 << (SparksqlParser.STAR - 327)) | (1 << (SparksqlParser.DIVIDE - 327)) | (1 << (SparksqlParser.MODULE - 327)))) != 0)):
                            localctx.op = self._errHandler.recoverInline(self)
                        else:
                            self.consume()
                        self.state = 436
                        self.expression(6)
                        pass

                    elif la_ == 2:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 437
                        if not self.precpred(self._ctx, 3):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 3)")
                        self.state = 438
                        localctx.op = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 330)) & ~0x3f) == 0 and ((1 << (_la - 330)) & ((1 << (SparksqlParser.PLUS - 330)) | (1 << (SparksqlParser.MINUS - 330)) | (1 << (SparksqlParser.BIT_OR - 330)) | (1 << (SparksqlParser.BIT_AND - 330)) | (1 << (SparksqlParser.BIT_XOR - 330)))) != 0)):
                            localctx.op = self._errHandler.recoverInline(self)
                        else:
                            self.consume()
                        self.state = 439
                        self.expression(4)
                        pass

                    elif la_ == 3:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 440
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 441
                        self.comparison_operator()
                        self.state = 442
                        self.expression(3)
                        pass

                    elif la_ == 4:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 444
                        if not self.precpred(self._ctx, 11):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 11)")
                        self.state = 445
                        self.match(SparksqlParser.COLLATE)
                        self.state = 446
                        self.id_1()
                        pass

             
                self.state = 451
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,55,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx

    class Aggregate_windowed_functionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def AVG(self):
            return self.getToken(SparksqlParser.AVG, 0)

        def all_distinct_expression(self):
            return self.getTypedRuleContext(SparksqlParser.All_distinct_expressionContext,0)


        def CHECKSUM_AGG(self):
            return self.getToken(SparksqlParser.CHECKSUM_AGG, 0)

        def GROUPING(self):
            return self.getToken(SparksqlParser.GROUPING, 0)

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def GROUPING_ID(self):
            return self.getToken(SparksqlParser.GROUPING_ID, 0)

        def expression_list(self):
            return self.getTypedRuleContext(SparksqlParser.Expression_listContext,0)


        def MAX(self):
            return self.getToken(SparksqlParser.MAX, 0)

        def MIN(self):
            return self.getToken(SparksqlParser.MIN, 0)

        def SUM(self):
            return self.getToken(SparksqlParser.SUM, 0)

        def STDEV(self):
            return self.getToken(SparksqlParser.STDEV, 0)

        def STDEVP(self):
            return self.getToken(SparksqlParser.STDEVP, 0)

        def VAR(self):
            return self.getToken(SparksqlParser.VAR, 0)

        def VARP(self):
            return self.getToken(SparksqlParser.VARP, 0)

        def COUNT(self):
            return self.getToken(SparksqlParser.COUNT, 0)

        def COUNT_BIG(self):
            return self.getToken(SparksqlParser.COUNT_BIG, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_aggregate_windowed_function

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAggregate_windowed_function" ):
                listener.enterAggregate_windowed_function(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAggregate_windowed_function" ):
                listener.exitAggregate_windowed_function(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAggregate_windowed_function" ):
                return visitor.visitAggregate_windowed_function(self)
            else:
                return visitor.visitChildren(self)




    def aggregate_windowed_function(self):

        localctx = SparksqlParser.Aggregate_windowed_functionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 58, self.RULE_aggregate_windowed_function)
        try:
            self.state = 521
            token = self._input.LA(1)
            if token in [SparksqlParser.AVG]:
                self.enterOuterAlt(localctx, 1)
                self.state = 452
                self.match(SparksqlParser.AVG)
                self.state = 453
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 454
                self.all_distinct_expression()
                self.state = 455
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.CHECKSUM_AGG]:
                self.enterOuterAlt(localctx, 2)
                self.state = 457
                self.match(SparksqlParser.CHECKSUM_AGG)
                self.state = 458
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 459
                self.all_distinct_expression()
                self.state = 460
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.GROUPING]:
                self.enterOuterAlt(localctx, 3)
                self.state = 462
                self.match(SparksqlParser.GROUPING)
                self.state = 463
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 464
                self.expression(0)
                self.state = 465
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.GROUPING_ID]:
                self.enterOuterAlt(localctx, 4)
                self.state = 467
                self.match(SparksqlParser.GROUPING_ID)
                self.state = 468
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 469
                self.expression_list()
                self.state = 470
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.MAX]:
                self.enterOuterAlt(localctx, 5)
                self.state = 472
                self.match(SparksqlParser.MAX)
                self.state = 473
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 474
                self.all_distinct_expression()
                self.state = 475
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.MIN]:
                self.enterOuterAlt(localctx, 6)
                self.state = 477
                self.match(SparksqlParser.MIN)
                self.state = 478
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 479
                self.all_distinct_expression()
                self.state = 480
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.SUM]:
                self.enterOuterAlt(localctx, 7)
                self.state = 482
                self.match(SparksqlParser.SUM)
                self.state = 483
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 484
                self.all_distinct_expression()
                self.state = 485
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.STDEV]:
                self.enterOuterAlt(localctx, 8)
                self.state = 487
                self.match(SparksqlParser.STDEV)
                self.state = 488
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 489
                self.all_distinct_expression()
                self.state = 490
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.STDEVP]:
                self.enterOuterAlt(localctx, 9)
                self.state = 492
                self.match(SparksqlParser.STDEVP)
                self.state = 493
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 494
                self.all_distinct_expression()
                self.state = 495
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.VAR]:
                self.enterOuterAlt(localctx, 10)
                self.state = 497
                self.match(SparksqlParser.VAR)
                self.state = 498
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 499
                self.all_distinct_expression()
                self.state = 500
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.VARP]:
                self.enterOuterAlt(localctx, 11)
                self.state = 502
                self.match(SparksqlParser.VARP)
                self.state = 503
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 504
                self.all_distinct_expression()
                self.state = 505
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.COUNT]:
                self.enterOuterAlt(localctx, 12)
                self.state = 507
                self.match(SparksqlParser.COUNT)
                self.state = 508
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 511
                token = self._input.LA(1)
                if token in [SparksqlParser.STAR]:
                    self.state = 509
                    self.match(SparksqlParser.STAR)

                elif token in [SparksqlParser.ALL, SparksqlParser.CASE, SparksqlParser.DEFAULT, SparksqlParser.DISTINCT, SparksqlParser.NULL, SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.DOUBLE_QUOTE_ID, SparksqlParser.SQUARE_BRACKET_ID, SparksqlParser.LOCAL_ID, SparksqlParser.DECIMAL, SparksqlParser.ID, SparksqlParser.STRING, SparksqlParser.BINARY, SparksqlParser.FLOAT, SparksqlParser.REAL, SparksqlParser.DOT, SparksqlParser.DOLLAR, SparksqlParser.LR_BRACKET, SparksqlParser.PLUS, SparksqlParser.MINUS, SparksqlParser.BIT_NOT]:
                    self.state = 510
                    self.all_distinct_expression()

                else:
                    raise NoViableAltException(self)

                self.state = 513
                self.match(SparksqlParser.RR_BRACKET)

            elif token in [SparksqlParser.COUNT_BIG]:
                self.enterOuterAlt(localctx, 13)
                self.state = 514
                self.match(SparksqlParser.COUNT_BIG)
                self.state = 515
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 518
                token = self._input.LA(1)
                if token in [SparksqlParser.STAR]:
                    self.state = 516
                    self.match(SparksqlParser.STAR)

                elif token in [SparksqlParser.ALL, SparksqlParser.CASE, SparksqlParser.DEFAULT, SparksqlParser.DISTINCT, SparksqlParser.NULL, SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.DOUBLE_QUOTE_ID, SparksqlParser.SQUARE_BRACKET_ID, SparksqlParser.LOCAL_ID, SparksqlParser.DECIMAL, SparksqlParser.ID, SparksqlParser.STRING, SparksqlParser.BINARY, SparksqlParser.FLOAT, SparksqlParser.REAL, SparksqlParser.DOT, SparksqlParser.DOLLAR, SparksqlParser.LR_BRACKET, SparksqlParser.PLUS, SparksqlParser.MINUS, SparksqlParser.BIT_NOT]:
                    self.state = 517
                    self.all_distinct_expression()

                else:
                    raise NoViableAltException(self)

                self.state = 520
                self.match(SparksqlParser.RR_BRACKET)

            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class All_distinct_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def DISTINCT(self):
            return self.getToken(SparksqlParser.DISTINCT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_all_distinct_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAll_distinct_expression" ):
                listener.enterAll_distinct_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAll_distinct_expression" ):
                listener.exitAll_distinct_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAll_distinct_expression" ):
                return visitor.visitAll_distinct_expression(self)
            else:
                return visitor.visitChildren(self)




    def all_distinct_expression(self):

        localctx = SparksqlParser.All_distinct_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 60, self.RULE_all_distinct_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 524
            _la = self._input.LA(1)
            if _la==SparksqlParser.ALL or _la==SparksqlParser.DISTINCT:
                self.state = 523
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ALL or _la==SparksqlParser.DISTINCT):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()


            self.state = 526
            self.expression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ConstantContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def STRING(self):
            return self.getToken(SparksqlParser.STRING, 0)

        def BINARY(self):
            return self.getToken(SparksqlParser.BINARY, 0)

        def number(self):
            return self.getTypedRuleContext(SparksqlParser.NumberContext,0)


        def REAL(self):
            return self.getToken(SparksqlParser.REAL, 0)

        def FLOAT(self):
            return self.getToken(SparksqlParser.FLOAT, 0)

        def sign(self):
            return self.getTypedRuleContext(SparksqlParser.SignContext,0)


        def DECIMAL(self):
            return self.getToken(SparksqlParser.DECIMAL, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_constant

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConstant" ):
                listener.enterConstant(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConstant" ):
                listener.exitConstant(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConstant" ):
                return visitor.visitConstant(self)
            else:
                return visitor.visitChildren(self)




    def constant(self):

        localctx = SparksqlParser.ConstantContext(self, self._ctx, self.state)
        self.enterRule(localctx, 62, self.RULE_constant)
        self._la = 0 # Token type
        try:
            self.state = 540
            la_ = self._interp.adaptivePredict(self._input,62,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 528
                self.match(SparksqlParser.STRING)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 529
                self.match(SparksqlParser.BINARY)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 530
                self.number()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 532
                _la = self._input.LA(1)
                if _la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS:
                    self.state = 531
                    self.sign()


                self.state = 534
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.FLOAT or _la==SparksqlParser.REAL):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 536
                _la = self._input.LA(1)
                if _la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS:
                    self.state = 535
                    self.sign()


                self.state = 538
                self.match(SparksqlParser.DOLLAR)
                self.state = 539
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.DECIMAL or _la==SparksqlParser.FLOAT):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SignContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def PLUS(self):
            return self.getToken(SparksqlParser.PLUS, 0)

        def MINUS(self):
            return self.getToken(SparksqlParser.MINUS, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_sign

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSign" ):
                listener.enterSign(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSign" ):
                listener.exitSign(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSign" ):
                return visitor.visitSign(self)
            else:
                return visitor.visitChildren(self)




    def sign(self):

        localctx = SparksqlParser.SignContext(self, self._ctx, self.state)
        self.enterRule(localctx, 64, self.RULE_sign)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 542
            _la = self._input.LA(1)
            if not(_la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS):
                self._errHandler.recoverInline(self)
            else:
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NumberContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def DECIMAL(self):
            return self.getToken(SparksqlParser.DECIMAL, 0)

        def sign(self):
            return self.getTypedRuleContext(SparksqlParser.SignContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_number

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNumber" ):
                listener.enterNumber(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNumber" ):
                listener.exitNumber(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNumber" ):
                return visitor.visitNumber(self)
            else:
                return visitor.visitChildren(self)




    def number(self):

        localctx = SparksqlParser.NumberContext(self, self._ctx, self.state)
        self.enterRule(localctx, 66, self.RULE_number)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 545
            _la = self._input.LA(1)
            if _la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS:
                self.state = 544
                self.sign()


            self.state = 547
            self.match(SparksqlParser.DECIMAL)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Select_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def select_list_elem(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Select_list_elemContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Select_list_elemContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_select_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelect_list" ):
                listener.enterSelect_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelect_list" ):
                listener.exitSelect_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelect_list" ):
                return visitor.visitSelect_list(self)
            else:
                return visitor.visitChildren(self)




    def select_list(self):

        localctx = SparksqlParser.Select_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 68, self.RULE_select_list)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 549
            self.select_list_elem()
            self.state = 554
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,64,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 550
                    self.match(SparksqlParser.COMMA)
                    self.state = 551
                    self.select_list_elem() 
                self.state = 556
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,64,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Select_list_elemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def IDENTITY(self):
            return self.getToken(SparksqlParser.IDENTITY, 0)

        def ROWGUID(self):
            return self.getToken(SparksqlParser.ROWGUID, 0)

        def column_alias(self):
            return self.getTypedRuleContext(SparksqlParser.Column_aliasContext,0)


        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def AS(self):
            return self.getToken(SparksqlParser.AS, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_select_list_elem

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelect_list_elem" ):
                listener.enterSelect_list_elem(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelect_list_elem" ):
                listener.exitSelect_list_elem(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelect_list_elem" ):
                return visitor.visitSelect_list_elem(self)
            else:
                return visitor.visitChildren(self)




    def select_list_elem(self):

        localctx = SparksqlParser.Select_list_elemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 70, self.RULE_select_list_elem)
        self._la = 0 # Token type
        try:
            self.state = 578
            la_ = self._interp.adaptivePredict(self._input,69,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 560
                _la = self._input.LA(1)
                if ((((_la - 187)) & ~0x3f) == 0 and ((1 << (_la - 187)) & ((1 << (SparksqlParser.ABSOLUTE - 187)) | (1 << (SparksqlParser.APPLY - 187)) | (1 << (SparksqlParser.AUTO - 187)) | (1 << (SparksqlParser.AVG - 187)) | (1 << (SparksqlParser.BASE64 - 187)) | (1 << (SparksqlParser.CALLER - 187)) | (1 << (SparksqlParser.CAST - 187)) | (1 << (SparksqlParser.CATCH - 187)) | (1 << (SparksqlParser.CHECKSUM_AGG - 187)) | (1 << (SparksqlParser.COMMITTED - 187)) | (1 << (SparksqlParser.CONCAT - 187)) | (1 << (SparksqlParser.COOKIE - 187)) | (1 << (SparksqlParser.COUNT - 187)) | (1 << (SparksqlParser.COUNT_BIG - 187)) | (1 << (SparksqlParser.DELAY - 187)) | (1 << (SparksqlParser.DELETED - 187)) | (1 << (SparksqlParser.DENSE_RANK - 187)) | (1 << (SparksqlParser.DISABLE - 187)) | (1 << (SparksqlParser.DYNAMIC - 187)) | (1 << (SparksqlParser.ENCRYPTION - 187)) | (1 << (SparksqlParser.FAST - 187)) | (1 << (SparksqlParser.FAST_FORWARD - 187)) | (1 << (SparksqlParser.FIRST - 187)) | (1 << (SparksqlParser.FOLLOWING - 187)) | (1 << (SparksqlParser.FORWARD_ONLY - 187)) | (1 << (SparksqlParser.FULLSCAN - 187)) | (1 << (SparksqlParser.GLOBAL - 187)) | (1 << (SparksqlParser.GO - 187)) | (1 << (SparksqlParser.GROUPING - 187)) | (1 << (SparksqlParser.GROUPING_ID - 187)) | (1 << (SparksqlParser.HASH - 187)) | (1 << (SparksqlParser.INSENSITIVE - 187)) | (1 << (SparksqlParser.INSERTED - 187)) | (1 << (SparksqlParser.ISOLATION - 187)) | (1 << (SparksqlParser.KEEPFIXED - 187)) | (1 << (SparksqlParser.KEYSET - 187)) | (1 << (SparksqlParser.LAST - 187)) | (1 << (SparksqlParser.LEVEL - 187)) | (1 << (SparksqlParser.LOCAL - 187)) | (1 << (SparksqlParser.LOCK_ESCALATION - 187)) | (1 << (SparksqlParser.LOGIN - 187)) | (1 << (SparksqlParser.LOOP - 187)) | (1 << (SparksqlParser.MARK - 187)) | (1 << (SparksqlParser.MAX - 187)) | (1 << (SparksqlParser.MIN - 187)) | (1 << (SparksqlParser.MODIFY - 187)) | (1 << (SparksqlParser.NEXT - 187)) | (1 << (SparksqlParser.NAME - 187)) | (1 << (SparksqlParser.NOCOUNT - 187)) | (1 << (SparksqlParser.NOEXPAND - 187)) | (1 << (SparksqlParser.NORECOMPUTE - 187)) | (1 << (SparksqlParser.NTILE - 187)) | (1 << (SparksqlParser.NUMBER - 187)) | (1 << (SparksqlParser.OFFSET - 187)) | (1 << (SparksqlParser.ONLY - 187)) | (1 << (SparksqlParser.OPTIMISTIC - 187)) | (1 << (SparksqlParser.OPTIMIZE - 187)) | (1 << (SparksqlParser.OUT - 187)) | (1 << (SparksqlParser.OUTPUT - 187)) | (1 << (SparksqlParser.OWNER - 187)) | (1 << (SparksqlParser.PARTITION - 187)) | (1 << (SparksqlParser.PATH - 187)) | (1 << (SparksqlParser.PRECEDING - 187)) | (1 << (SparksqlParser.PRIOR - 187)))) != 0) or ((((_la - 251)) & ~0x3f) == 0 and ((1 << (_la - 251)) & ((1 << (SparksqlParser.RANGE - 251)) | (1 << (SparksqlParser.RANK - 251)) | (1 << (SparksqlParser.READONLY - 251)) | (1 << (SparksqlParser.READ_ONLY - 251)) | (1 << (SparksqlParser.RECOMPILE - 251)) | (1 << (SparksqlParser.RELATIVE - 251)) | (1 << (SparksqlParser.REMOTE - 251)) | (1 << (SparksqlParser.REPEATABLE - 251)) | (1 << (SparksqlParser.ROOT - 251)) | (1 << (SparksqlParser.ROW - 251)) | (1 << (SparksqlParser.ROWGUID - 251)) | (1 << (SparksqlParser.ROWS - 251)) | (1 << (SparksqlParser.ROW_NUMBER - 251)) | (1 << (SparksqlParser.SAMPLE - 251)) | (1 << (SparksqlParser.SCHEMABINDING - 251)) | (1 << (SparksqlParser.SCROLL - 251)) | (1 << (SparksqlParser.SCROLL_LOCKS - 251)) | (1 << (SparksqlParser.SELF - 251)) | (1 << (SparksqlParser.SERIALIZABLE - 251)) | (1 << (SparksqlParser.SNAPSHOT - 251)) | (1 << (SparksqlParser.STATIC - 251)) | (1 << (SparksqlParser.STATS_STREAM - 251)) | (1 << (SparksqlParser.STDEV - 251)) | (1 << (SparksqlParser.STDEVP - 251)) | (1 << (SparksqlParser.SUM - 251)) | (1 << (SparksqlParser.THROW - 251)) | (1 << (SparksqlParser.TIES - 251)) | (1 << (SparksqlParser.TIME - 251)) | (1 << (SparksqlParser.TRY - 251)) | (1 << (SparksqlParser.TYPE - 251)) | (1 << (SparksqlParser.TYPE_WARNING - 251)) | (1 << (SparksqlParser.UNBOUNDED - 251)) | (1 << (SparksqlParser.UNCOMMITTED - 251)) | (1 << (SparksqlParser.UNKNOWN - 251)) | (1 << (SparksqlParser.USING - 251)) | (1 << (SparksqlParser.VAR - 251)) | (1 << (SparksqlParser.VARP - 251)) | (1 << (SparksqlParser.VIEW_METADATA - 251)) | (1 << (SparksqlParser.WORK - 251)) | (1 << (SparksqlParser.XML - 251)) | (1 << (SparksqlParser.XMLNAMESPACES - 251)) | (1 << (SparksqlParser.DOUBLE_QUOTE_ID - 251)) | (1 << (SparksqlParser.SQUARE_BRACKET_ID - 251)) | (1 << (SparksqlParser.ID - 251)))) != 0) or _la==SparksqlParser.DOT:
                    self.state = 557
                    self.table_name()
                    self.state = 558
                    self.match(SparksqlParser.DOT)


                self.state = 565
                token = self._input.LA(1)
                if token in [SparksqlParser.STAR]:
                    self.state = 562
                    self.match(SparksqlParser.STAR)

                elif token in [SparksqlParser.DOLLAR]:
                    self.state = 563
                    self.match(SparksqlParser.DOLLAR)
                    self.state = 564
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.IDENTITY or _la==SparksqlParser.ROWGUID):
                        self._errHandler.recoverInline(self)
                    else:
                        self.consume()

                else:
                    raise NoViableAltException(self)

                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 567
                self.column_alias()
                self.state = 568
                self.match(SparksqlParser.EQUAL)
                self.state = 569
                self.expression(0)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 571
                self.expression(0)
                self.state = 576
                la_ = self._interp.adaptivePredict(self._input,68,self._ctx)
                if la_ == 1:
                    self.state = 573
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.AS:
                        self.state = 572
                        self.match(SparksqlParser.AS)


                    self.state = 575
                    self.column_alias()


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Full_column_nameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def column_name(self):
            return self.getTypedRuleContext(SparksqlParser.Column_nameContext,0)


        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_full_column_name

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFull_column_name" ):
                listener.enterFull_column_name(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFull_column_name" ):
                listener.exitFull_column_name(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFull_column_name" ):
                return visitor.visitFull_column_name(self)
            else:
                return visitor.visitChildren(self)




    def full_column_name(self):

        localctx = SparksqlParser.Full_column_nameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 72, self.RULE_full_column_name)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 583
            la_ = self._interp.adaptivePredict(self._input,70,self._ctx)
            if la_ == 1:
                self.state = 580
                self.table_name()
                self.state = 581
                self.match(SparksqlParser.DOT)


            self.state = 585
            self.column_name()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_nameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_column_name

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_name" ):
                listener.enterColumn_name(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_name" ):
                listener.exitColumn_name(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_name" ):
                return visitor.visitColumn_name(self)
            else:
                return visitor.visitChildren(self)




    def column_name(self):

        localctx = SparksqlParser.Column_nameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 74, self.RULE_column_name)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 587
            self.id_1()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Case_exprContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def CASE(self):
            return self.getToken(SparksqlParser.CASE, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def END(self):
            return self.getToken(SparksqlParser.END, 0)

        def WHEN(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.WHEN)
            else:
                return self.getToken(SparksqlParser.WHEN, i)

        def THEN(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.THEN)
            else:
                return self.getToken(SparksqlParser.THEN, i)

        def ELSE(self):
            return self.getToken(SparksqlParser.ELSE, 0)

        def search_condition(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_conditionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_case_expr

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCase_expr" ):
                listener.enterCase_expr(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCase_expr" ):
                listener.exitCase_expr(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCase_expr" ):
                return visitor.visitCase_expr(self)
            else:
                return visitor.visitChildren(self)




    def case_expr(self):

        localctx = SparksqlParser.Case_exprContext(self, self._ctx, self.state)
        self.enterRule(localctx, 76, self.RULE_case_expr)
        self._la = 0 # Token type
        try:
            self.state = 622
            la_ = self._interp.adaptivePredict(self._input,75,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 589
                self.match(SparksqlParser.CASE)
                self.state = 590
                self.expression(0)
                self.state = 596 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 591
                    self.match(SparksqlParser.WHEN)
                    self.state = 592
                    self.expression(0)
                    self.state = 593
                    self.match(SparksqlParser.THEN)
                    self.state = 594
                    self.expression(0)
                    self.state = 598 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparksqlParser.WHEN):
                        break

                self.state = 602
                _la = self._input.LA(1)
                if _la==SparksqlParser.ELSE:
                    self.state = 600
                    self.match(SparksqlParser.ELSE)
                    self.state = 601
                    self.expression(0)


                self.state = 604
                self.match(SparksqlParser.END)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 606
                self.match(SparksqlParser.CASE)
                self.state = 612 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 607
                    self.match(SparksqlParser.WHEN)
                    self.state = 608
                    self.search_condition()
                    self.state = 609
                    self.match(SparksqlParser.THEN)
                    self.state = 610
                    self.expression(0)
                    self.state = 614 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparksqlParser.WHEN):
                        break

                self.state = 618
                _la = self._input.LA(1)
                if _la==SparksqlParser.ELSE:
                    self.state = 616
                    self.match(SparksqlParser.ELSE)
                    self.state = 617
                    self.expression(0)


                self.state = 620
                self.match(SparksqlParser.END)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_nameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.schema = None # Id_1Context
            self.table = None # Id_1Context

        def id_1(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Id_1Context)
            else:
                return self.getTypedRuleContext(SparksqlParser.Id_1Context,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_name

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_name" ):
                listener.enterTable_name(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_name" ):
                listener.exitTable_name(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_name" ):
                return visitor.visitTable_name(self)
            else:
                return visitor.visitChildren(self)




    def table_name(self):

        localctx = SparksqlParser.Table_nameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 78, self.RULE_table_name)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 631
            la_ = self._interp.adaptivePredict(self._input,77,self._ctx)
            if la_ == 1:
                self.state = 625
                _la = self._input.LA(1)
                if ((((_la - 187)) & ~0x3f) == 0 and ((1 << (_la - 187)) & ((1 << (SparksqlParser.ABSOLUTE - 187)) | (1 << (SparksqlParser.APPLY - 187)) | (1 << (SparksqlParser.AUTO - 187)) | (1 << (SparksqlParser.AVG - 187)) | (1 << (SparksqlParser.BASE64 - 187)) | (1 << (SparksqlParser.CALLER - 187)) | (1 << (SparksqlParser.CAST - 187)) | (1 << (SparksqlParser.CATCH - 187)) | (1 << (SparksqlParser.CHECKSUM_AGG - 187)) | (1 << (SparksqlParser.COMMITTED - 187)) | (1 << (SparksqlParser.CONCAT - 187)) | (1 << (SparksqlParser.COOKIE - 187)) | (1 << (SparksqlParser.COUNT - 187)) | (1 << (SparksqlParser.COUNT_BIG - 187)) | (1 << (SparksqlParser.DELAY - 187)) | (1 << (SparksqlParser.DELETED - 187)) | (1 << (SparksqlParser.DENSE_RANK - 187)) | (1 << (SparksqlParser.DISABLE - 187)) | (1 << (SparksqlParser.DYNAMIC - 187)) | (1 << (SparksqlParser.ENCRYPTION - 187)) | (1 << (SparksqlParser.FAST - 187)) | (1 << (SparksqlParser.FAST_FORWARD - 187)) | (1 << (SparksqlParser.FIRST - 187)) | (1 << (SparksqlParser.FOLLOWING - 187)) | (1 << (SparksqlParser.FORWARD_ONLY - 187)) | (1 << (SparksqlParser.FULLSCAN - 187)) | (1 << (SparksqlParser.GLOBAL - 187)) | (1 << (SparksqlParser.GO - 187)) | (1 << (SparksqlParser.GROUPING - 187)) | (1 << (SparksqlParser.GROUPING_ID - 187)) | (1 << (SparksqlParser.HASH - 187)) | (1 << (SparksqlParser.INSENSITIVE - 187)) | (1 << (SparksqlParser.INSERTED - 187)) | (1 << (SparksqlParser.ISOLATION - 187)) | (1 << (SparksqlParser.KEEPFIXED - 187)) | (1 << (SparksqlParser.KEYSET - 187)) | (1 << (SparksqlParser.LAST - 187)) | (1 << (SparksqlParser.LEVEL - 187)) | (1 << (SparksqlParser.LOCAL - 187)) | (1 << (SparksqlParser.LOCK_ESCALATION - 187)) | (1 << (SparksqlParser.LOGIN - 187)) | (1 << (SparksqlParser.LOOP - 187)) | (1 << (SparksqlParser.MARK - 187)) | (1 << (SparksqlParser.MAX - 187)) | (1 << (SparksqlParser.MIN - 187)) | (1 << (SparksqlParser.MODIFY - 187)) | (1 << (SparksqlParser.NEXT - 187)) | (1 << (SparksqlParser.NAME - 187)) | (1 << (SparksqlParser.NOCOUNT - 187)) | (1 << (SparksqlParser.NOEXPAND - 187)) | (1 << (SparksqlParser.NORECOMPUTE - 187)) | (1 << (SparksqlParser.NTILE - 187)) | (1 << (SparksqlParser.NUMBER - 187)) | (1 << (SparksqlParser.OFFSET - 187)) | (1 << (SparksqlParser.ONLY - 187)) | (1 << (SparksqlParser.OPTIMISTIC - 187)) | (1 << (SparksqlParser.OPTIMIZE - 187)) | (1 << (SparksqlParser.OUT - 187)) | (1 << (SparksqlParser.OUTPUT - 187)) | (1 << (SparksqlParser.OWNER - 187)) | (1 << (SparksqlParser.PARTITION - 187)) | (1 << (SparksqlParser.PATH - 187)) | (1 << (SparksqlParser.PRECEDING - 187)) | (1 << (SparksqlParser.PRIOR - 187)))) != 0) or ((((_la - 251)) & ~0x3f) == 0 and ((1 << (_la - 251)) & ((1 << (SparksqlParser.RANGE - 251)) | (1 << (SparksqlParser.RANK - 251)) | (1 << (SparksqlParser.READONLY - 251)) | (1 << (SparksqlParser.READ_ONLY - 251)) | (1 << (SparksqlParser.RECOMPILE - 251)) | (1 << (SparksqlParser.RELATIVE - 251)) | (1 << (SparksqlParser.REMOTE - 251)) | (1 << (SparksqlParser.REPEATABLE - 251)) | (1 << (SparksqlParser.ROOT - 251)) | (1 << (SparksqlParser.ROW - 251)) | (1 << (SparksqlParser.ROWGUID - 251)) | (1 << (SparksqlParser.ROWS - 251)) | (1 << (SparksqlParser.ROW_NUMBER - 251)) | (1 << (SparksqlParser.SAMPLE - 251)) | (1 << (SparksqlParser.SCHEMABINDING - 251)) | (1 << (SparksqlParser.SCROLL - 251)) | (1 << (SparksqlParser.SCROLL_LOCKS - 251)) | (1 << (SparksqlParser.SELF - 251)) | (1 << (SparksqlParser.SERIALIZABLE - 251)) | (1 << (SparksqlParser.SNAPSHOT - 251)) | (1 << (SparksqlParser.STATIC - 251)) | (1 << (SparksqlParser.STATS_STREAM - 251)) | (1 << (SparksqlParser.STDEV - 251)) | (1 << (SparksqlParser.STDEVP - 251)) | (1 << (SparksqlParser.SUM - 251)) | (1 << (SparksqlParser.THROW - 251)) | (1 << (SparksqlParser.TIES - 251)) | (1 << (SparksqlParser.TIME - 251)) | (1 << (SparksqlParser.TRY - 251)) | (1 << (SparksqlParser.TYPE - 251)) | (1 << (SparksqlParser.TYPE_WARNING - 251)) | (1 << (SparksqlParser.UNBOUNDED - 251)) | (1 << (SparksqlParser.UNCOMMITTED - 251)) | (1 << (SparksqlParser.UNKNOWN - 251)) | (1 << (SparksqlParser.USING - 251)) | (1 << (SparksqlParser.VAR - 251)) | (1 << (SparksqlParser.VARP - 251)) | (1 << (SparksqlParser.VIEW_METADATA - 251)) | (1 << (SparksqlParser.WORK - 251)) | (1 << (SparksqlParser.XML - 251)) | (1 << (SparksqlParser.XMLNAMESPACES - 251)) | (1 << (SparksqlParser.DOUBLE_QUOTE_ID - 251)) | (1 << (SparksqlParser.SQUARE_BRACKET_ID - 251)) | (1 << (SparksqlParser.ID - 251)))) != 0):
                    self.state = 624
                    localctx.schema = self.id_1()


                self.state = 627
                self.match(SparksqlParser.DOT)

            elif la_ == 2:
                self.state = 628
                localctx.schema = self.id_1()
                self.state = 629
                self.match(SparksqlParser.DOT)


            self.state = 633
            localctx.table = self.id_1()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Search_conditionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def search_condition_or(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_condition_orContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_condition_orContext,i)


        def AND(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.AND)
            else:
                return self.getToken(SparksqlParser.AND, i)

        def getRuleIndex(self):
            return SparksqlParser.RULE_search_condition

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearch_condition" ):
                listener.enterSearch_condition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearch_condition" ):
                listener.exitSearch_condition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearch_condition" ):
                return visitor.visitSearch_condition(self)
            else:
                return visitor.visitChildren(self)




    def search_condition(self):

        localctx = SparksqlParser.Search_conditionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 80, self.RULE_search_condition)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 635
            self.search_condition_or()
            self.state = 640
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.AND:
                self.state = 636
                self.match(SparksqlParser.AND)
                self.state = 637
                self.search_condition_or()
                self.state = 642
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Search_condition_orContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def search_condition_not(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_condition_notContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_condition_notContext,i)


        def OR(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.OR)
            else:
                return self.getToken(SparksqlParser.OR, i)

        def getRuleIndex(self):
            return SparksqlParser.RULE_search_condition_or

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearch_condition_or" ):
                listener.enterSearch_condition_or(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearch_condition_or" ):
                listener.exitSearch_condition_or(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearch_condition_or" ):
                return visitor.visitSearch_condition_or(self)
            else:
                return visitor.visitChildren(self)




    def search_condition_or(self):

        localctx = SparksqlParser.Search_condition_orContext(self, self._ctx, self.state)
        self.enterRule(localctx, 82, self.RULE_search_condition_or)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 643
            self.search_condition_not()
            self.state = 648
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.OR:
                self.state = 644
                self.match(SparksqlParser.OR)
                self.state = 645
                self.search_condition_not()
                self.state = 650
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Search_condition_notContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def predicate(self):
            return self.getTypedRuleContext(SparksqlParser.PredicateContext,0)


        def NOT(self):
            return self.getToken(SparksqlParser.NOT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_search_condition_not

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearch_condition_not" ):
                listener.enterSearch_condition_not(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearch_condition_not" ):
                listener.exitSearch_condition_not(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearch_condition_not" ):
                return visitor.visitSearch_condition_not(self)
            else:
                return visitor.visitChildren(self)




    def search_condition_not(self):

        localctx = SparksqlParser.Search_condition_notContext(self, self._ctx, self.state)
        self.enterRule(localctx, 84, self.RULE_search_condition_not)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 652
            _la = self._input.LA(1)
            if _la==SparksqlParser.NOT:
                self.state = 651
                self.match(SparksqlParser.NOT)


            self.state = 654
            self.predicate()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PredicateContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def EXISTS(self):
            return self.getToken(SparksqlParser.EXISTS, 0)

        def subquery(self):
            return self.getTypedRuleContext(SparksqlParser.SubqueryContext,0)


        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def comparison_operator(self):
            return self.getTypedRuleContext(SparksqlParser.Comparison_operatorContext,0)


        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def SOME(self):
            return self.getToken(SparksqlParser.SOME, 0)

        def ANY(self):
            return self.getToken(SparksqlParser.ANY, 0)

        def BETWEEN(self):
            return self.getToken(SparksqlParser.BETWEEN, 0)

        def AND(self):
            return self.getToken(SparksqlParser.AND, 0)

        def NOT(self):
            return self.getToken(SparksqlParser.NOT, 0)

        def IN(self):
            return self.getToken(SparksqlParser.IN, 0)

        def expression_list(self):
            return self.getTypedRuleContext(SparksqlParser.Expression_listContext,0)


        def LIKE(self):
            return self.getToken(SparksqlParser.LIKE, 0)

        def ESCAPE(self):
            return self.getToken(SparksqlParser.ESCAPE, 0)

        def IS(self):
            return self.getToken(SparksqlParser.IS, 0)

        def null_notnull(self):
            return self.getTypedRuleContext(SparksqlParser.Null_notnullContext,0)


        def search_condition(self):
            return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_predicate

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicate" ):
                listener.enterPredicate(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicate" ):
                listener.exitPredicate(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicate" ):
                return visitor.visitPredicate(self)
            else:
                return visitor.visitChildren(self)




    def predicate(self):

        localctx = SparksqlParser.PredicateContext(self, self._ctx, self.state)
        self.enterRule(localctx, 86, self.RULE_predicate)
        self._la = 0 # Token type
        try:
            self.state = 711
            la_ = self._interp.adaptivePredict(self._input,86,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 656
                self.match(SparksqlParser.EXISTS)
                self.state = 657
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 658
                self.subquery()
                self.state = 659
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 661
                self.expression(0)
                self.state = 662
                self.comparison_operator()
                self.state = 663
                self.expression(0)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 665
                self.expression(0)
                self.state = 666
                self.comparison_operator()
                self.state = 667
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ALL or _la==SparksqlParser.ANY or _la==SparksqlParser.SOME):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                self.state = 668
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 669
                self.subquery()
                self.state = 670
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 672
                self.expression(0)
                self.state = 674
                _la = self._input.LA(1)
                if _la==SparksqlParser.NOT:
                    self.state = 673
                    self.match(SparksqlParser.NOT)


                self.state = 676
                self.match(SparksqlParser.BETWEEN)
                self.state = 677
                self.expression(0)
                self.state = 678
                self.match(SparksqlParser.AND)
                self.state = 679
                self.expression(0)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 681
                self.expression(0)
                self.state = 683
                _la = self._input.LA(1)
                if _la==SparksqlParser.NOT:
                    self.state = 682
                    self.match(SparksqlParser.NOT)


                self.state = 685
                self.match(SparksqlParser.IN)
                self.state = 686
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 689
                la_ = self._interp.adaptivePredict(self._input,83,self._ctx)
                if la_ == 1:
                    self.state = 687
                    self.subquery()
                    pass

                elif la_ == 2:
                    self.state = 688
                    self.expression_list()
                    pass


                self.state = 691
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 693
                self.expression(0)
                self.state = 695
                _la = self._input.LA(1)
                if _la==SparksqlParser.NOT:
                    self.state = 694
                    self.match(SparksqlParser.NOT)


                self.state = 697
                self.match(SparksqlParser.LIKE)
                self.state = 698
                self.expression(0)
                self.state = 701
                _la = self._input.LA(1)
                if _la==SparksqlParser.ESCAPE:
                    self.state = 699
                    self.match(SparksqlParser.ESCAPE)
                    self.state = 700
                    self.expression(0)


                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 703
                self.expression(0)
                self.state = 704
                self.match(SparksqlParser.IS)
                self.state = 705
                self.null_notnull()
                pass

            elif la_ == 8:
                self.enterOuterAlt(localctx, 8)
                self.state = 707
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 708
                self.search_condition()
                self.state = 709
                self.match(SparksqlParser.RR_BRACKET)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Id_1Context(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def simple_id(self):
            return self.getTypedRuleContext(SparksqlParser.Simple_idContext,0)


        def DOUBLE_QUOTE_ID(self):
            return self.getToken(SparksqlParser.DOUBLE_QUOTE_ID, 0)

        def SQUARE_BRACKET_ID(self):
            return self.getToken(SparksqlParser.SQUARE_BRACKET_ID, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_id_1

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterId_1" ):
                listener.enterId_1(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitId_1" ):
                listener.exitId_1(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitId_1" ):
                return visitor.visitId_1(self)
            else:
                return visitor.visitChildren(self)




    def id_1(self):

        localctx = SparksqlParser.Id_1Context(self, self._ctx, self.state)
        self.enterRule(localctx, 88, self.RULE_id_1)
        try:
            self.state = 716
            token = self._input.LA(1)
            if token in [SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.ID]:
                self.enterOuterAlt(localctx, 1)
                self.state = 713
                self.simple_id()

            elif token in [SparksqlParser.DOUBLE_QUOTE_ID]:
                self.enterOuterAlt(localctx, 2)
                self.state = 714
                self.match(SparksqlParser.DOUBLE_QUOTE_ID)

            elif token in [SparksqlParser.SQUARE_BRACKET_ID]:
                self.enterOuterAlt(localctx, 3)
                self.state = 715
                self.match(SparksqlParser.SQUARE_BRACKET_ID)

            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Comparison_operatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparksqlParser.RULE_comparison_operator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComparison_operator" ):
                listener.enterComparison_operator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComparison_operator" ):
                listener.exitComparison_operator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComparison_operator" ):
                return visitor.visitComparison_operator(self)
            else:
                return visitor.visitChildren(self)




    def comparison_operator(self):

        localctx = SparksqlParser.Comparison_operatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 90, self.RULE_comparison_operator)
        try:
            self.state = 733
            la_ = self._interp.adaptivePredict(self._input,88,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 718
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 719
                self.match(SparksqlParser.GREATER)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 720
                self.match(SparksqlParser.LESS)
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 721
                self.match(SparksqlParser.LESS)
                self.state = 722
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 723
                self.match(SparksqlParser.GREATER)
                self.state = 724
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 725
                self.match(SparksqlParser.LESS)
                self.state = 726
                self.match(SparksqlParser.GREATER)
                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 727
                self.match(SparksqlParser.EXCLAMATION)
                self.state = 728
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 8:
                self.enterOuterAlt(localctx, 8)
                self.state = 729
                self.match(SparksqlParser.EXCLAMATION)
                self.state = 730
                self.match(SparksqlParser.GREATER)
                pass

            elif la_ == 9:
                self.enterOuterAlt(localctx, 9)
                self.state = 731
                self.match(SparksqlParser.EXCLAMATION)
                self.state = 732
                self.match(SparksqlParser.LESS)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Simple_idContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ID(self):
            return self.getToken(SparksqlParser.ID, 0)

        def ABSOLUTE(self):
            return self.getToken(SparksqlParser.ABSOLUTE, 0)

        def APPLY(self):
            return self.getToken(SparksqlParser.APPLY, 0)

        def AUTO(self):
            return self.getToken(SparksqlParser.AUTO, 0)

        def AVG(self):
            return self.getToken(SparksqlParser.AVG, 0)

        def BASE64(self):
            return self.getToken(SparksqlParser.BASE64, 0)

        def CALLER(self):
            return self.getToken(SparksqlParser.CALLER, 0)

        def CAST(self):
            return self.getToken(SparksqlParser.CAST, 0)

        def CATCH(self):
            return self.getToken(SparksqlParser.CATCH, 0)

        def CHECKSUM_AGG(self):
            return self.getToken(SparksqlParser.CHECKSUM_AGG, 0)

        def COMMITTED(self):
            return self.getToken(SparksqlParser.COMMITTED, 0)

        def CONCAT(self):
            return self.getToken(SparksqlParser.CONCAT, 0)

        def COOKIE(self):
            return self.getToken(SparksqlParser.COOKIE, 0)

        def COUNT(self):
            return self.getToken(SparksqlParser.COUNT, 0)

        def COUNT_BIG(self):
            return self.getToken(SparksqlParser.COUNT_BIG, 0)

        def DELAY(self):
            return self.getToken(SparksqlParser.DELAY, 0)

        def DELETED(self):
            return self.getToken(SparksqlParser.DELETED, 0)

        def DENSE_RANK(self):
            return self.getToken(SparksqlParser.DENSE_RANK, 0)

        def DISABLE(self):
            return self.getToken(SparksqlParser.DISABLE, 0)

        def DYNAMIC(self):
            return self.getToken(SparksqlParser.DYNAMIC, 0)

        def ENCRYPTION(self):
            return self.getToken(SparksqlParser.ENCRYPTION, 0)

        def FAST(self):
            return self.getToken(SparksqlParser.FAST, 0)

        def FAST_FORWARD(self):
            return self.getToken(SparksqlParser.FAST_FORWARD, 0)

        def FIRST(self):
            return self.getToken(SparksqlParser.FIRST, 0)

        def FOLLOWING(self):
            return self.getToken(SparksqlParser.FOLLOWING, 0)

        def FORWARD_ONLY(self):
            return self.getToken(SparksqlParser.FORWARD_ONLY, 0)

        def FULLSCAN(self):
            return self.getToken(SparksqlParser.FULLSCAN, 0)

        def GLOBAL(self):
            return self.getToken(SparksqlParser.GLOBAL, 0)

        def GO(self):
            return self.getToken(SparksqlParser.GO, 0)

        def GROUPING(self):
            return self.getToken(SparksqlParser.GROUPING, 0)

        def GROUPING_ID(self):
            return self.getToken(SparksqlParser.GROUPING_ID, 0)

        def HASH(self):
            return self.getToken(SparksqlParser.HASH, 0)

        def INSENSITIVE(self):
            return self.getToken(SparksqlParser.INSENSITIVE, 0)

        def INSERTED(self):
            return self.getToken(SparksqlParser.INSERTED, 0)

        def ISOLATION(self):
            return self.getToken(SparksqlParser.ISOLATION, 0)

        def KEYSET(self):
            return self.getToken(SparksqlParser.KEYSET, 0)

        def KEEPFIXED(self):
            return self.getToken(SparksqlParser.KEEPFIXED, 0)

        def LAST(self):
            return self.getToken(SparksqlParser.LAST, 0)

        def LEVEL(self):
            return self.getToken(SparksqlParser.LEVEL, 0)

        def LOCAL(self):
            return self.getToken(SparksqlParser.LOCAL, 0)

        def LOCK_ESCALATION(self):
            return self.getToken(SparksqlParser.LOCK_ESCALATION, 0)

        def LOGIN(self):
            return self.getToken(SparksqlParser.LOGIN, 0)

        def LOOP(self):
            return self.getToken(SparksqlParser.LOOP, 0)

        def MARK(self):
            return self.getToken(SparksqlParser.MARK, 0)

        def MAX(self):
            return self.getToken(SparksqlParser.MAX, 0)

        def MIN(self):
            return self.getToken(SparksqlParser.MIN, 0)

        def MODIFY(self):
            return self.getToken(SparksqlParser.MODIFY, 0)

        def NAME(self):
            return self.getToken(SparksqlParser.NAME, 0)

        def NEXT(self):
            return self.getToken(SparksqlParser.NEXT, 0)

        def NOCOUNT(self):
            return self.getToken(SparksqlParser.NOCOUNT, 0)

        def NOEXPAND(self):
            return self.getToken(SparksqlParser.NOEXPAND, 0)

        def NORECOMPUTE(self):
            return self.getToken(SparksqlParser.NORECOMPUTE, 0)

        def NTILE(self):
            return self.getToken(SparksqlParser.NTILE, 0)

        def NUMBER(self):
            return self.getToken(SparksqlParser.NUMBER, 0)

        def OFFSET(self):
            return self.getToken(SparksqlParser.OFFSET, 0)

        def ONLY(self):
            return self.getToken(SparksqlParser.ONLY, 0)

        def OPTIMISTIC(self):
            return self.getToken(SparksqlParser.OPTIMISTIC, 0)

        def OPTIMIZE(self):
            return self.getToken(SparksqlParser.OPTIMIZE, 0)

        def OUT(self):
            return self.getToken(SparksqlParser.OUT, 0)

        def OUTPUT(self):
            return self.getToken(SparksqlParser.OUTPUT, 0)

        def OWNER(self):
            return self.getToken(SparksqlParser.OWNER, 0)

        def PARTITION(self):
            return self.getToken(SparksqlParser.PARTITION, 0)

        def PATH(self):
            return self.getToken(SparksqlParser.PATH, 0)

        def PRECEDING(self):
            return self.getToken(SparksqlParser.PRECEDING, 0)

        def PRIOR(self):
            return self.getToken(SparksqlParser.PRIOR, 0)

        def RANGE(self):
            return self.getToken(SparksqlParser.RANGE, 0)

        def RANK(self):
            return self.getToken(SparksqlParser.RANK, 0)

        def READONLY(self):
            return self.getToken(SparksqlParser.READONLY, 0)

        def READ_ONLY(self):
            return self.getToken(SparksqlParser.READ_ONLY, 0)

        def RECOMPILE(self):
            return self.getToken(SparksqlParser.RECOMPILE, 0)

        def RELATIVE(self):
            return self.getToken(SparksqlParser.RELATIVE, 0)

        def REMOTE(self):
            return self.getToken(SparksqlParser.REMOTE, 0)

        def REPEATABLE(self):
            return self.getToken(SparksqlParser.REPEATABLE, 0)

        def ROOT(self):
            return self.getToken(SparksqlParser.ROOT, 0)

        def ROW(self):
            return self.getToken(SparksqlParser.ROW, 0)

        def ROWGUID(self):
            return self.getToken(SparksqlParser.ROWGUID, 0)

        def ROWS(self):
            return self.getToken(SparksqlParser.ROWS, 0)

        def ROW_NUMBER(self):
            return self.getToken(SparksqlParser.ROW_NUMBER, 0)

        def SAMPLE(self):
            return self.getToken(SparksqlParser.SAMPLE, 0)

        def SCHEMABINDING(self):
            return self.getToken(SparksqlParser.SCHEMABINDING, 0)

        def SCROLL(self):
            return self.getToken(SparksqlParser.SCROLL, 0)

        def SCROLL_LOCKS(self):
            return self.getToken(SparksqlParser.SCROLL_LOCKS, 0)

        def SELF(self):
            return self.getToken(SparksqlParser.SELF, 0)

        def SERIALIZABLE(self):
            return self.getToken(SparksqlParser.SERIALIZABLE, 0)

        def SNAPSHOT(self):
            return self.getToken(SparksqlParser.SNAPSHOT, 0)

        def STATIC(self):
            return self.getToken(SparksqlParser.STATIC, 0)

        def STATS_STREAM(self):
            return self.getToken(SparksqlParser.STATS_STREAM, 0)

        def STDEV(self):
            return self.getToken(SparksqlParser.STDEV, 0)

        def STDEVP(self):
            return self.getToken(SparksqlParser.STDEVP, 0)

        def SUM(self):
            return self.getToken(SparksqlParser.SUM, 0)

        def THROW(self):
            return self.getToken(SparksqlParser.THROW, 0)

        def TIES(self):
            return self.getToken(SparksqlParser.TIES, 0)

        def TIME(self):
            return self.getToken(SparksqlParser.TIME, 0)

        def TRY(self):
            return self.getToken(SparksqlParser.TRY, 0)

        def TYPE(self):
            return self.getToken(SparksqlParser.TYPE, 0)

        def TYPE_WARNING(self):
            return self.getToken(SparksqlParser.TYPE_WARNING, 0)

        def UNBOUNDED(self):
            return self.getToken(SparksqlParser.UNBOUNDED, 0)

        def UNCOMMITTED(self):
            return self.getToken(SparksqlParser.UNCOMMITTED, 0)

        def UNKNOWN(self):
            return self.getToken(SparksqlParser.UNKNOWN, 0)

        def USING(self):
            return self.getToken(SparksqlParser.USING, 0)

        def VAR(self):
            return self.getToken(SparksqlParser.VAR, 0)

        def VARP(self):
            return self.getToken(SparksqlParser.VARP, 0)

        def VIEW_METADATA(self):
            return self.getToken(SparksqlParser.VIEW_METADATA, 0)

        def WORK(self):
            return self.getToken(SparksqlParser.WORK, 0)

        def XML(self):
            return self.getToken(SparksqlParser.XML, 0)

        def XMLNAMESPACES(self):
            return self.getToken(SparksqlParser.XMLNAMESPACES, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_simple_id

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSimple_id" ):
                listener.enterSimple_id(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSimple_id" ):
                listener.exitSimple_id(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSimple_id" ):
                return visitor.visitSimple_id(self)
            else:
                return visitor.visitChildren(self)




    def simple_id(self):

        localctx = SparksqlParser.Simple_idContext(self, self._ctx, self.state)
        self.enterRule(localctx, 92, self.RULE_simple_id)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 735
            _la = self._input.LA(1)
            if not(((((_la - 187)) & ~0x3f) == 0 and ((1 << (_la - 187)) & ((1 << (SparksqlParser.ABSOLUTE - 187)) | (1 << (SparksqlParser.APPLY - 187)) | (1 << (SparksqlParser.AUTO - 187)) | (1 << (SparksqlParser.AVG - 187)) | (1 << (SparksqlParser.BASE64 - 187)) | (1 << (SparksqlParser.CALLER - 187)) | (1 << (SparksqlParser.CAST - 187)) | (1 << (SparksqlParser.CATCH - 187)) | (1 << (SparksqlParser.CHECKSUM_AGG - 187)) | (1 << (SparksqlParser.COMMITTED - 187)) | (1 << (SparksqlParser.CONCAT - 187)) | (1 << (SparksqlParser.COOKIE - 187)) | (1 << (SparksqlParser.COUNT - 187)) | (1 << (SparksqlParser.COUNT_BIG - 187)) | (1 << (SparksqlParser.DELAY - 187)) | (1 << (SparksqlParser.DELETED - 187)) | (1 << (SparksqlParser.DENSE_RANK - 187)) | (1 << (SparksqlParser.DISABLE - 187)) | (1 << (SparksqlParser.DYNAMIC - 187)) | (1 << (SparksqlParser.ENCRYPTION - 187)) | (1 << (SparksqlParser.FAST - 187)) | (1 << (SparksqlParser.FAST_FORWARD - 187)) | (1 << (SparksqlParser.FIRST - 187)) | (1 << (SparksqlParser.FOLLOWING - 187)) | (1 << (SparksqlParser.FORWARD_ONLY - 187)) | (1 << (SparksqlParser.FULLSCAN - 187)) | (1 << (SparksqlParser.GLOBAL - 187)) | (1 << (SparksqlParser.GO - 187)) | (1 << (SparksqlParser.GROUPING - 187)) | (1 << (SparksqlParser.GROUPING_ID - 187)) | (1 << (SparksqlParser.HASH - 187)) | (1 << (SparksqlParser.INSENSITIVE - 187)) | (1 << (SparksqlParser.INSERTED - 187)) | (1 << (SparksqlParser.ISOLATION - 187)) | (1 << (SparksqlParser.KEEPFIXED - 187)) | (1 << (SparksqlParser.KEYSET - 187)) | (1 << (SparksqlParser.LAST - 187)) | (1 << (SparksqlParser.LEVEL - 187)) | (1 << (SparksqlParser.LOCAL - 187)) | (1 << (SparksqlParser.LOCK_ESCALATION - 187)) | (1 << (SparksqlParser.LOGIN - 187)) | (1 << (SparksqlParser.LOOP - 187)) | (1 << (SparksqlParser.MARK - 187)) | (1 << (SparksqlParser.MAX - 187)) | (1 << (SparksqlParser.MIN - 187)) | (1 << (SparksqlParser.MODIFY - 187)) | (1 << (SparksqlParser.NEXT - 187)) | (1 << (SparksqlParser.NAME - 187)) | (1 << (SparksqlParser.NOCOUNT - 187)) | (1 << (SparksqlParser.NOEXPAND - 187)) | (1 << (SparksqlParser.NORECOMPUTE - 187)) | (1 << (SparksqlParser.NTILE - 187)) | (1 << (SparksqlParser.NUMBER - 187)) | (1 << (SparksqlParser.OFFSET - 187)) | (1 << (SparksqlParser.ONLY - 187)) | (1 << (SparksqlParser.OPTIMISTIC - 187)) | (1 << (SparksqlParser.OPTIMIZE - 187)) | (1 << (SparksqlParser.OUT - 187)) | (1 << (SparksqlParser.OUTPUT - 187)) | (1 << (SparksqlParser.OWNER - 187)) | (1 << (SparksqlParser.PARTITION - 187)) | (1 << (SparksqlParser.PATH - 187)) | (1 << (SparksqlParser.PRECEDING - 187)) | (1 << (SparksqlParser.PRIOR - 187)))) != 0) or ((((_la - 251)) & ~0x3f) == 0 and ((1 << (_la - 251)) & ((1 << (SparksqlParser.RANGE - 251)) | (1 << (SparksqlParser.RANK - 251)) | (1 << (SparksqlParser.READONLY - 251)) | (1 << (SparksqlParser.READ_ONLY - 251)) | (1 << (SparksqlParser.RECOMPILE - 251)) | (1 << (SparksqlParser.RELATIVE - 251)) | (1 << (SparksqlParser.REMOTE - 251)) | (1 << (SparksqlParser.REPEATABLE - 251)) | (1 << (SparksqlParser.ROOT - 251)) | (1 << (SparksqlParser.ROW - 251)) | (1 << (SparksqlParser.ROWGUID - 251)) | (1 << (SparksqlParser.ROWS - 251)) | (1 << (SparksqlParser.ROW_NUMBER - 251)) | (1 << (SparksqlParser.SAMPLE - 251)) | (1 << (SparksqlParser.SCHEMABINDING - 251)) | (1 << (SparksqlParser.SCROLL - 251)) | (1 << (SparksqlParser.SCROLL_LOCKS - 251)) | (1 << (SparksqlParser.SELF - 251)) | (1 << (SparksqlParser.SERIALIZABLE - 251)) | (1 << (SparksqlParser.SNAPSHOT - 251)) | (1 << (SparksqlParser.STATIC - 251)) | (1 << (SparksqlParser.STATS_STREAM - 251)) | (1 << (SparksqlParser.STDEV - 251)) | (1 << (SparksqlParser.STDEVP - 251)) | (1 << (SparksqlParser.SUM - 251)) | (1 << (SparksqlParser.THROW - 251)) | (1 << (SparksqlParser.TIES - 251)) | (1 << (SparksqlParser.TIME - 251)) | (1 << (SparksqlParser.TRY - 251)) | (1 << (SparksqlParser.TYPE - 251)) | (1 << (SparksqlParser.TYPE_WARNING - 251)) | (1 << (SparksqlParser.UNBOUNDED - 251)) | (1 << (SparksqlParser.UNCOMMITTED - 251)) | (1 << (SparksqlParser.UNKNOWN - 251)) | (1 << (SparksqlParser.USING - 251)) | (1 << (SparksqlParser.VAR - 251)) | (1 << (SparksqlParser.VARP - 251)) | (1 << (SparksqlParser.VIEW_METADATA - 251)) | (1 << (SparksqlParser.WORK - 251)) | (1 << (SparksqlParser.XML - 251)) | (1 << (SparksqlParser.XMLNAMESPACES - 251)) | (1 << (SparksqlParser.ID - 251)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Null_notnullContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def NULL(self):
            return self.getToken(SparksqlParser.NULL, 0)

        def NOT(self):
            return self.getToken(SparksqlParser.NOT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_null_notnull

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNull_notnull" ):
                listener.enterNull_notnull(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNull_notnull" ):
                listener.exitNull_notnull(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNull_notnull" ):
                return visitor.visitNull_notnull(self)
            else:
                return visitor.visitChildren(self)




    def null_notnull(self):

        localctx = SparksqlParser.Null_notnullContext(self, self._ctx, self.state)
        self.enterRule(localctx, 94, self.RULE_null_notnull)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 738
            _la = self._input.LA(1)
            if _la==SparksqlParser.NOT:
                self.state = 737
                self.match(SparksqlParser.NOT)


            self.state = 740
            self.match(SparksqlParser.NULL)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx



    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):
        if self._predicates == None:
            self._predicates = dict()
        self._predicates[28] = self.expression_sempred
        pred = self._predicates.get(ruleIndex, None)
        if pred is None:
            raise Exception("No predicate with index:" + str(ruleIndex))
        else:
            return pred(localctx, predIndex)

    def expression_sempred(self, localctx:ExpressionContext, predIndex:int):
            if predIndex == 0:
                return self.precpred(self._ctx, 5)
         

            if predIndex == 1:
                return self.precpred(self._ctx, 3)
         

            if predIndex == 2:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 3:
                return self.precpred(self._ctx, 11)
         




